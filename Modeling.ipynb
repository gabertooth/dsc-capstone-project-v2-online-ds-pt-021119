{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re #Regular Expression package\n",
    "import pandas as pd #Pandas dataframes and etc.\n",
    "import numpy as np #NUmpy package\n",
    "import matplotlib.pyplot as plt #Matplotlib graphing package\n",
    "%matplotlib inline\n",
    "# from sklearn.manifold import TSNE\n",
    "import nltk #NLTK NLP Package\n",
    "from nltk.tokenize import word_tokenize #NLTK NLP Package\n",
    "np.random.seed(0)\n",
    "from nltk.corpus import gutenberg, stopwords #NLTK NLP Package\n",
    "from nltk.collocations import * #NLTK NLP Package\n",
    "from nltk import FreqDist #NLTK NLP Package\n",
    "from nltk import word_tokenize #NLTK NLP Package\n",
    "from nltk.stem.porter import * #NLTK Stemming\n",
    "from nltk.stem import * #NLTK Lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split #Sklearn modeling etc. \n",
    "from sklearn.feature_extraction.text import CountVectorizer #Sklearn modeling etc.\n",
    "from sklearn.linear_model import LogisticRegression #Sklearn modeling etc.\n",
    "import seaborn as sns #Seaborn\n",
    "from keras.preprocessing.text import Tokenizer #Keras Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences #Keras padding\n",
    "from keras.models import Sequential #Keras Sequential Decl.\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D #Neural Networks\n",
    "from keras.utils.np_utils import to_categorical #Encoding labeling\n",
    "from keras.callbacks import EarlyStopping #Keras early stopback\n",
    "from keras.layers import Dropout #Dropout\n",
    "import re #Regular Expression\n",
    "import string #String package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('January-October')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') #Stop words from english language\n",
    "stopwords_list += list(string.punctuation) #String characters \n",
    "stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\"’\", '‘','said','says', \"'s\",'”','“'] #Miscelleneous and numerical characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_lowercase=['i','me','my','myself','we','our','ours','ourselves',\n",
    " 'you', \"you're\", \"you've\", \"you'll\",\"you'd\", 'your','yours', 'yourself',\n",
    "'yourselves', 'he','him','his', 'himself','she', \"she's\",\n",
    " 'her', 'hers','herself','it', \"it's\",'its', 'itself','they', 'them','their', 'theirs',\n",
    " 'themselves','what', 'which','who', 'whom','this', 'that',\n",
    " \"that'll\", 'these','those', 'am','is', 'are','was', 'were',\n",
    " 'be', 'been','being', 'have', 'has', 'had','having', 'do',\n",
    " 'does', 'did','doing', 'a', 'an', 'the','and', 'but',\n",
    " 'if', 'or','because','as','until', 'while','of','at','by', 'for','with','about','against',\n",
    " 'between','into','through','during', 'before','after','above','below',\n",
    " 'to','from','up','down', 'in','out','on','off',\n",
    " 'over','under','again','further', 'then','once','here','there',\n",
    " 'when','where','why','how', 'all','any','both','each', 'few','more',\n",
    " 'most','other', 'some','such', 'no','nor', 'not','only',\n",
    " 'own','same', 'so','than', 'too','very', 's','t',\n",
    " 'can','will','just','don', \"don't\",'should', \"should've\",'now',\n",
    " 'd','ll', 'm','o', 're','ve', 'y','ain',\n",
    " 'aren',\"aren't\", 'couldn',\"couldn't\", 'didn',\"didn't\", 'doesn',\"doesn't\",\n",
    " 'hadn',\"hadn't\", 'hasn',\"hasn't\", 'haven',\"haven't\", 'isn',\"isn't\",\n",
    " 'ma','mightn', \"mightn't\",'mustn', \"mustn't\",'needn', \"needn't\",'shan',\n",
    " \"shan't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",\n",
    " 'weren',\"weren't\", 'won',\"won't\", 'wouldn',\"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in stopwords_lowercase: #Decided to include a for loop to capatilize the stop words due to capitalization in headings. \n",
    "    stopwords_list.append(x.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=df['Text'].apply(nltk.word_tokenize)\n",
    "df_tokens=[] \n",
    "for x in tokenized_df: #Stop word removal along with lower casing the tokens\n",
    "    for y in x:\n",
    "        if y not in stopwords_list:\n",
    "            df_tokens.append(y.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text'] #Splitting into X and y variables\n",
    "y=df['Coded']\n",
    "\n",
    "sent_train, sent_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=1) #Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= CountVectorizer() #Vectorization object\n",
    "vectorizer.fit(sent_train)\n",
    "\n",
    "X_train= vectorizer.transform(sent_train) #transforming vectors\n",
    "X_test= vectorizer.transform(sent_test) #transforming vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6455203116304953\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression() #Classifier\n",
    "classifier.fit(X_train, y_train) #Fitting model\n",
    "score = classifier.score(X_test, y_test) #Accuracy Score\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic tokenization was used and achieved accuracy of 64 percent. The two paths forward are as follow: make the model more complex or tokenize the words differently by using bigrams or lemmentization. Hopefully, by isolating the models and only tweaking one aspect of the data/moddel we will improve our accuracy.\n",
    "\n",
    "First I am going to try to change the tokenized inputs of the models:\n",
    "    1. Stemming and Lemmentization\n",
    "    2. Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmentization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>Change</th>\n",
       "      <th>Coded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Trump Invites Top Lawmakers in Effort To End S...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Kim Jong Un Extends Peace Overture to U.S.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>American Detained in Russia Isn’t a Spy, Famil...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>The Money Managers to Watch in 2019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Investors Try Not to Panic Over Stock Volatility</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               Text  Change  \\\n",
       "0  2019-01-01  Trump Invites Top Lawmakers in Effort To End S...     1.0   \n",
       "1  2019-01-01         Kim Jong Un Extends Peace Overture to U.S.     1.0   \n",
       "2  2019-01-01  American Detained in Russia Isn’t a Spy, Famil...     1.0   \n",
       "3  2019-01-01                The Money Managers to Watch in 2019     1.0   \n",
       "4  2019-01-01   Investors Try Not to Panic Over Stock Volatility     1.0   \n",
       "\n",
       "   Coded  \n",
       "0    1.0  \n",
       "1    1.0  \n",
       "2    1.0  \n",
       "3    1.0  \n",
       "4    1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #Loading original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #Creating the PorterStemmer\n",
    "stemmed_words= [stemmer.stem(token) for token in df_tokens] #Stemming the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('new', 2688),\n",
       " ('u.s.', 2493),\n",
       " ('trump', 2073),\n",
       " ('compani', 1346),\n",
       " ('million', 1261),\n",
       " ('year', 1141),\n",
       " ('presid', 1089),\n",
       " ('say', 1088),\n",
       " ('report', 1082),\n",
       " ('hous', 1069)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(stemmed_words).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There really is not a difference between the stemmed words vs. the traditional tokenized word counts. Now lets run a model to see if this had any effect on the modeling accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input): #Defining a function to be able to load this into our model as the tokenizer\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [PorterStemmer().stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text']\n",
    "y=df['Coded']\n",
    "\n",
    "sent_train, sent_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=1)\n",
    "\n",
    "vectorizer= CountVectorizer(tokenizer=stemming_tokenizer) #Everything else is the same as passed models, except tweaking tokenizer\n",
    "vectorizer.fit(sent_train)\n",
    "\n",
    "X_train= vectorizer.transform(sent_train)\n",
    "X_test= vectorizer.transform(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.92 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", round(score*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, stemming did not improve our model by very much. Essentially it improved by a little under a percentage point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeration(x):\n",
    "    for x in df_tokens:\n",
    "        word= []\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        word.append(lemmatizer.lemmatize(x))\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text']\n",
    "y=df['Coded']\n",
    "\n",
    "sent_train, sent_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=1)\n",
    "\n",
    "vectorizer= CountVectorizer(tokenizer=lemmatizeration)\n",
    "vectorizer.fit(sent_train)\n",
    "\n",
    "X_train= vectorizer.transform(sent_train)\n",
    "X_test= vectorizer.transform(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.93 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", round(score*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing the text gave us a identical result to stemming. I am not confident that changing the mode of text will help the model, but I will try one last strategy, Bigrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy for tokenizing is using bigrams, which are words that essentailly belong or show up together in sentances. The computer is able to recongnize which words belong together and which ones do not. Below is the analysis of the bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('president', 'trump'), 0.0017477505802716874),\n",
       " (('new', 'york'), 0.0016830190772986618),\n",
       " (('logistics', 'report'), 0.0009216533042349815),\n",
       " (('today', 'logistics'), 0.0009216533042349815),\n",
       " (('trump', 'administration'), 0.0009154883991899314),\n",
       " (('hong', 'kong'), 0.0008939112315322563),\n",
       " (('white', 'house'), 0.0008939112315322563),\n",
       " (('wall', 'street'), 0.0006473150297302546),\n",
       " (('morning', 'risk'), 0.0006010782418923793),\n",
       " (('risk', 'report'), 0.0006010782418923793),\n",
       " (('737', 'max'), 0.0005579239065770289),\n",
       " (('morning', 'download'), 0.0004901099510814785),\n",
       " (('york', 'city'), 0.0004746976884688534),\n",
       " (('supreme', 'court'), 0.0004407907107210781),\n",
       " (('prime', 'minister'), 0.0004315433531535031),\n",
       " (('great', 'escapes'), 0.000422295995585928),\n",
       " (('justice', 'department'), 0.00040380128045077785),\n",
       " (('last', 'year'), 0.0003791416602705777),\n",
       " (('north', 'korea'), 0.0003760592077480527),\n",
       " (('paid', 'program'), 0.00037297675522552764),\n",
       " (('pg', 'e'), 0.0003575644926129025),\n",
       " (('federal', 'reserve'), 0.0003390697774777524),\n",
       " (('chief', 'executive'), 0.0003359873249552274),\n",
       " (('los', 'angeles'), 0.00032982241991017734),\n",
       " (('saudi', 'arabia'), 0.0003020803472074521),\n",
       " (('attorney', 'general'), 0.00029591544216240207),\n",
       " (('analyst', 'says'), 0.0002928329896398771),\n",
       " (('san', 'francisco'), 0.0002928329896398771),\n",
       " (('world', 'cup'), 0.00028975053711735203),\n",
       " (('cmo', 'today'), 0.000283585632072302),\n",
       " (('trade', 'talks'), 0.000283585632072302),\n",
       " (('super', 'bowl'), 0.000280503179549777),\n",
       " (('interest', 'rates'), 0.00027742072702725194),\n",
       " (('today', 'newsletter'), 0.00027433827450472695),\n",
       " (('islamic', 'state'), 0.0002681733694596769),\n",
       " (('week', 'ahead'), 0.0002681733694596769),\n",
       " (('u.s.', 'stocks'), 0.00026200846441462686),\n",
       " (('economy', 'week'), 0.0002589260118921018),\n",
       " (('new', 'cfo'), 0.00025276110684705177),\n",
       " (('real', 'estate'), 0.00025276110684705177),\n",
       " (('rate', 'cut'), 0.0002496786543245268),\n",
       " (('finance', 'chief'), 0.0002465962018020017),\n",
       " (('central', 'bank'), 0.0002404312967569517),\n",
       " (('deutsche', 'bank'), 0.0002404312967569517),\n",
       " (('mueller', 'report'), 0.0002404312967569517),\n",
       " (('trade', 'deal'), 0.00023118393918937665),\n",
       " (('joe', 'biden'), 0.0002250190341443266),\n",
       " (('names', 'new'), 0.0002250190341443266),\n",
       " (('first', 'time'), 0.00022193658162180158),\n",
       " (('democratic', 'presidential'), 0.00021885412909927656)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures() #Bigrams created and scored\n",
    "wsj_finder = BigramCollocationFinder.from_words(df_tokens)\n",
    "wsj_scored = wsj_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "wsj_scored[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI score: measure of association used in information theory and statistics referring to single events.\n",
    "\n",
    "Below are the calculations and code for this. It is very interesting due to the fact that it can call out important events that have happened over the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('abdelaziz', 'bouteflika'), 15.985561800747938),\n",
       " (('clayton', 'kershaw'), 15.985561800747938),\n",
       " (('edith', 'wharton'), 15.985561800747938),\n",
       " (('f.h', 'bertling'), 15.985561800747938),\n",
       " (('fire-', 'paparazzi-proofed'), 15.985561800747938),\n",
       " (('harriet', 'tubman'), 15.985561800747938),\n",
       " (('history—and', 'odds—are'), 15.985561800747938),\n",
       " (('javad', 'zarif'), 15.985561800747938),\n",
       " (('johns', 'hopkins'), 15.985561800747938),\n",
       " (('karlie', 'kloss'), 15.985561800747938)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj_pmi_finder = BigramCollocationFinder.from_words(df_tokens) #PMI Score creation\n",
    "wsj_pmi_finder.apply_freq_filter(5)\n",
    "wsj_pmi_scored = wsj_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
    "wsj_pmi_scored[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Abdelaziz Bouteflika: He was the President of Algeria and in 2019 he was removed from office by stepping down due to mass protests. \n",
    "2. Clayton Kershaw: A pitcher for the Los Angeles Dodgers who has struggled as of late in the post season. \n",
    "3. Nipsey Hussel: Famous Compton rapper who was killed in broad day light outside of his Marathon store. This was a huge deal for the rap and music community and touched many peopls hearts. \n",
    "4. Achilles Temdpm: Most likely in reference to the many athletes over 2019 that had achilles injuries. Just to name a few: John Wall, Kevin Durant, Dez Bryant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text'] #X and y declared\n",
    "y=df['Coded']\n",
    "\n",
    "sent_train, sent_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=1) #Train test split\n",
    "\n",
    "vectorizer= CountVectorizer(ngram_range=(2,2), stop_words='english') #Passing in bigram arguement\n",
    "vectorizer.fit(sent_train)\n",
    "\n",
    "X_train= vectorizer.transform(sent_train)\n",
    "X_test= vectorizer.transform(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.08 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression() #Defining and fiting the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", round(score*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigrams did in fact improve our model by around 4 or 5 percent! The next step is to use neural networks and other more advanced modeling techniques. The following sections will focus on this. I am hoping to get at least a 5-10 percent increase in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest accuracy so far came from our last model, using bigrams and logistic regression. I want to try to implement a bit more complex models in the hope to improve our accruacy substantially. First I will start with a basic neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('January-October') #loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27479 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 5000 # Max number of word\n",
    "MAX_SEQUENCE_LENGTH = 250 # This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True) #Tokenizer intialization\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "word_index = tokenizer.word_index #Tokenizing into index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (35938, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Text']) #Putting data into data tensor with padding to be able to feed into model\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (35938, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['Coded']) #one hot dummy encoding for y labeling purposes \n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28750, 250) (28750, 2)\n",
      "(7188, 250) (7188, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42) #train test split\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 580,602\n",
      "Trainable params: 580,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 153s 6ms/step - loss: 0.6462 - acc: 0.6520 - val_loss: 0.6493 - val_acc: 0.6417\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 159s 6ms/step - loss: 0.6094 - acc: 0.6709 - val_loss: 0.6575 - val_acc: 0.6379\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 158s 6ms/step - loss: 0.5573 - acc: 0.7123 - val_loss: 0.6817 - val_acc: 0.6217\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 159s 6ms/step - loss: 0.5164 - acc: 0.7435 - val_loss: 0.7072 - val_acc: 0.6170\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 165s 6ms/step - loss: 0.4785 - acc: 0.7705 - val_loss: 0.7632 - val_acc: 0.6197\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 10s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.744\n",
      "  Accuracy: 0.628\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a very high loss which means we are overfitting as the training data has max accuracy of 77 percent but the highest valadation accuracy is 64%. I am going to reduce the max number of words to try to narrow down our data to the most imporant words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS=2500 #Reducing max numbder of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27479 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True) #Rerunning to fit ot model\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (35938, 250)\n",
      "Shape of label tensor: (35938, 2)\n",
      "(28750, 250) (28750, 2)\n",
      "(7188, 250) (7188, 2)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Text']) #Rerunning for the lesser max_NB_WORDS variable\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "Y = pd.get_dummies(df['Coded'])\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 808       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 331,226\n",
      "Trainable params: 331,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 187s 7ms/step - loss: 0.6481 - acc: 0.6513 - val_loss: 0.6543 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 196s 8ms/step - loss: 0.6359 - acc: 0.6528 - val_loss: 0.6517 - val_acc: 0.6390\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 175s 7ms/step - loss: 0.6100 - acc: 0.6692 - val_loss: 0.6655 - val_acc: 0.6242\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 157s 6ms/step - loss: 0.5861 - acc: 0.6876 - val_loss: 0.6707 - val_acc: 0.6092\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 157s 6ms/step - loss: 0.5675 - acc: 0.7048 - val_loss: 0.6872 - val_acc: 0.5970\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 9s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.677\n",
      "  Accuracy: 0.616\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this substantially brought down the loss score to .687 which is around .244 lower. This is a step in a right direction. I would like to add another layer to see how this affects the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 332,170\n",
      "Trainable params: 332,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 208s 8ms/step - loss: 0.6468 - acc: 0.6521 - val_loss: 0.6518 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 203s 8ms/step - loss: 0.6258 - acc: 0.6576 - val_loss: 0.6562 - val_acc: 0.6259\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 208s 8ms/step - loss: 0.5971 - acc: 0.6813 - val_loss: 0.6603 - val_acc: 0.6174\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 208s 8ms/step - loss: 0.5781 - acc: 0.6988 - val_loss: 0.6852 - val_acc: 0.6197\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 209s 8ms/step - loss: 0.5587 - acc: 0.7139 - val_loss: 0.6852 - val_acc: 0.6235\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 10s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.678\n",
      "  Accuracy: 0.631\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did not help either of our metrics and ended up decreasing the accuracy and increasing the loss slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 334,978\n",
      "Trainable params: 334,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 222s 9ms/step - loss: 0.6477 - acc: 0.6521 - val_loss: 0.6531 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 223s 9ms/step - loss: 0.6274 - acc: 0.6559 - val_loss: 0.6542 - val_acc: 0.6280\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 221s 9ms/step - loss: 0.6016 - acc: 0.6776 - val_loss: 0.6695 - val_acc: 0.6216\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 223s 9ms/step - loss: 0.5787 - acc: 0.7000 - val_loss: 0.6715 - val_acc: 0.6146\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 221s 9ms/step - loss: 0.5548 - acc: 0.7184 - val_loss: 0.7010 - val_acc: 0.6219\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 10s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.693\n",
      "  Accuracy: 0.630\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 2056      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 617,642\n",
      "Trainable params: 617,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 553s 21ms/step - loss: 0.6468 - acc: 0.6525 - val_loss: 0.6511 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 565s 22ms/step - loss: 0.6213 - acc: 0.6631 - val_loss: 0.6554 - val_acc: 0.6310\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 564s 22ms/step - loss: 0.5938 - acc: 0.6850 - val_loss: 0.6608 - val_acc: 0.6174\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 563s 22ms/step - loss: 0.5933 - acc: 0.6861 - val_loss: 0.7001 - val_acc: 0.6137\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 564s 22ms/step - loss: 0.5682 - acc: 0.7076 - val_loss: 0.6891 - val_acc: 0.6322\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 60s 8ms/step\n",
      "Test set\n",
      "  Loss: 0.679\n",
      "  Accuracy: 0.640\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial and error with tweaking the model, changing the word counts and also adding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27479 unique tokens.\n",
      "Shape of data tensor: (35938, 250)\n",
      "Shape of label tensor: (35938, 2)\n",
      "(28750, 250) (28750, 2)\n",
      "(7188, 250) (7188, 2)\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS=2500\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "X = tokenizer.texts_to_sequences(df['Text'])\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "Y = pd.get_dummies(df['Coded'])\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 120)               106080    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 968       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(120, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 346s 13ms/step - loss: 0.6484 - acc: 0.6478 - val_loss: 0.6535 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 528s 20ms/step - loss: 0.6383 - acc: 0.6529 - val_loss: 0.6558 - val_acc: 0.6379\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 438s 17ms/step - loss: 0.6163 - acc: 0.6677 - val_loss: 0.6606 - val_acc: 0.6083\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 438s 17ms/step - loss: 0.5959 - acc: 0.6879 - val_loss: 0.6688 - val_acc: 0.6108\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 463s 18ms/step - loss: 0.5784 - acc: 0.7022 - val_loss: 0.6805 - val_acc: 0.6125\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 23s 3ms/step\n",
      "Test set\n",
      "  Loss: 0.669\n",
      "  Accuracy: 0.629\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another layer to see if this can improve any of the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 120)               106080    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                1936      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 358,170\n",
      "Trainable params: 358,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(120, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 451s 17ms/step - loss: 0.6872 - acc: 0.5922 - val_loss: 0.6548 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 490s 19ms/step - loss: 0.6462 - acc: 0.6526 - val_loss: 0.6543 - val_acc: 0.6390\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 579s 22ms/step - loss: 0.6459 - acc: 0.6526 - val_loss: 0.6546 - val_acc: 0.6390\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 579s 22ms/step - loss: 0.6459 - acc: 0.6526 - val_loss: 0.6543 - val_acc: 0.6390\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 459s 18ms/step - loss: 0.6459 - acc: 0.6526 - val_loss: 0.6543 - val_acc: 0.6390\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 31s 4ms/step\n",
      "Test set\n",
      "  Loss: 0.647\n",
      "  Accuracy: 0.651\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another layer! So far it has little affect on both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 120)               106080    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 24)                2904      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 359,538\n",
      "Trainable params: 359,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(120, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(24, activation='sigmoid'))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 569s 22ms/step - loss: 0.6474 - acc: 0.6526 - val_loss: 0.6545 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 471s 18ms/step - loss: 0.6460 - acc: 0.6526 - val_loss: 0.6543 - val_acc: 0.6390\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 553s 21ms/step - loss: 0.6459 - acc: 0.6526 - val_loss: 0.6542 - val_acc: 0.6390\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 563s 22ms/step - loss: 0.6452 - acc: 0.6526 - val_loss: 0.6537 - val_acc: 0.6390\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 600s 23ms/step - loss: 0.6320 - acc: 0.6526 - val_loss: 0.6554 - val_acc: 0.6390\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 32s 4ms/step\n",
      "Test set\n",
      "  Loss: 0.650\n",
      "  Accuracy: 0.651\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that another layer does not improve our accuracy and really only decreases our loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the modeling are as follows:  \n",
    "\n",
    "__Logistic Regression__  \n",
    "Baseline:\n",
    "Accuracy - 64.55%  \n",
    "Lemmatization:\n",
    "Accuracy - 64.92%  \n",
    "Stemming:\n",
    "Accuracy - 64.92%  \n",
    "Bigrams:\n",
    "Accuracy - 69.08%  \n",
    "\n",
    "\n",
    "__Neural Networks__  \n",
    "Baseline:  \n",
    "    - Accuracy: 62.80%  \n",
    "    - Loss: .744    \n",
    "Optimal Model:  \n",
    "    - Accuracy: 65.10%  \n",
    "    - Loss: .650  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Best Model:__   \n",
    "Bigram Logistic Regression: This model achieved around 69 percent accuracy in predicting the direction of the S&P 500 for the following day.  \n",
    "Neural networks were not sufficient and due to the black box component, the optimal number of layers were not found\n",
    "\n",
    "__NLP Takeaways from WSJ:__   \n",
    "\n",
    "Trump’s name was mentioned the most both in negative and positive days  \n",
    "There was not a real difference between the top words on a positive vs. negative days  \n",
    "Amazon and Apple were the two companies mentioned the most  \n",
    "\n",
    "__Further Analysis:__   \n",
    "Statistical testing to really find out what makes a positive vs. negative day  \n",
    "Time series modeling of Amazon and Apple to see whether the stock market went up or down when mentioned  \n",
    "Stacked modeling  \n",
    "Gather more data from the waybackmachine to see if we can improve the modeling  \n",
    "Lastly, compare Trump's presidency(2016-2020) to Obamas latter tenure (2012-2016) to compare word distributions. Does the media/ewspapers cover Trump more than any other president in history or was Obama covered just as much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

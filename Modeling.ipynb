{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #Regular Expression package\n",
    "import pandas as pd #Pandas dataframes and etc.\n",
    "import numpy as np #NUmpy package\n",
    "import matplotlib.pyplot as plt #Matplotlib graphing package\n",
    "%matplotlib inline\n",
    "# from sklearn.manifold import TSNE\n",
    "import nltk #NLTK NLP Package\n",
    "from nltk.tokenize import word_tokenize #NLTK NLP Package\n",
    "np.random.seed(0)\n",
    "from nltk.corpus import gutenberg, stopwords #NLTK NLP Package\n",
    "from nltk.collocations import * #NLTK NLP Package\n",
    "from nltk import FreqDist #NLTK NLP Package\n",
    "from nltk import word_tokenize #NLTK NLP Package\n",
    "from nltk.stem.porter import * #NLTK Stemming\n",
    "from nltk.stem import * #NLTK Lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split #Sklearn modeling etc. \n",
    "from sklearn.feature_extraction.text import CountVectorizer #Sklearn modeling etc.\n",
    "from sklearn.linear_model import LogisticRegression #Sklearn modeling etc.\n",
    "import seaborn as sns #Seaborn\n",
    "from keras.preprocessing.text import Tokenizer #Keras Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences #Keras padding\n",
    "from keras.models import Sequential #Keras Sequential Decl.\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D #Neural Networks\n",
    "from keras.utils.np_utils import to_categorical #Encoding labeling\n",
    "from keras.callbacks import EarlyStopping #Keras early stopback\n",
    "from keras.layers import Dropout #Dropout\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('January-October')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') #Stop words from english language\n",
    "stopwords_list += list(string.punctuation) #String characters \n",
    "stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\"’\", '‘','said','says', \"'s\",'”','“'] #Miscelleneous and numerical characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_lowercase=['i','me','my','myself','we','our','ours','ourselves',\n",
    " 'you', \"you're\", \"you've\", \"you'll\",\"you'd\", 'your','yours', 'yourself',\n",
    "'yourselves', 'he','him','his', 'himself','she', \"she's\",\n",
    " 'her', 'hers','herself','it', \"it's\",'its', 'itself','they', 'them','their', 'theirs',\n",
    " 'themselves','what', 'which','who', 'whom','this', 'that',\n",
    " \"that'll\", 'these','those', 'am','is', 'are','was', 'were',\n",
    " 'be', 'been','being', 'have', 'has', 'had','having', 'do',\n",
    " 'does', 'did','doing', 'a', 'an', 'the','and', 'but',\n",
    " 'if', 'or','because','as','until', 'while','of','at','by', 'for','with','about','against',\n",
    " 'between','into','through','during', 'before','after','above','below',\n",
    " 'to','from','up','down', 'in','out','on','off',\n",
    " 'over','under','again','further', 'then','once','here','there',\n",
    " 'when','where','why','how', 'all','any','both','each', 'few','more',\n",
    " 'most','other', 'some','such', 'no','nor', 'not','only',\n",
    " 'own','same', 'so','than', 'too','very', 's','t',\n",
    " 'can','will','just','don', \"don't\",'should', \"should've\",'now',\n",
    " 'd','ll', 'm','o', 're','ve', 'y','ain',\n",
    " 'aren',\"aren't\", 'couldn',\"couldn't\", 'didn',\"didn't\", 'doesn',\"doesn't\",\n",
    " 'hadn',\"hadn't\", 'hasn',\"hasn't\", 'haven',\"haven't\", 'isn',\"isn't\",\n",
    " 'ma','mightn', \"mightn't\",'mustn', \"mustn't\",'needn', \"needn't\",'shan',\n",
    " \"shan't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",\n",
    " 'weren',\"weren't\", 'won',\"won't\", 'wouldn',\"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in stopwords_lowercase: #Decided to include a for loop to capatilize the stop words due to capitalization in headings. \n",
    "    stopwords_list.append(x.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=df['Text'].apply(nltk.word_tokenize)\n",
    "df_tokens=[] \n",
    "for x in tokenized_df: #Stop word removal along with lower casing the tokens\n",
    "    for y in x:\n",
    "        if y not in stopwords_list:\n",
    "            df_tokens.append(y.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text'] #Splitting into X and y variables\n",
    "y=df['Coded']\n",
    "\n",
    "sent_train, sent_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=1) #Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer= CountVectorizer() #Vectorization object\n",
    "vectorizer.fit(sent_train)\n",
    "\n",
    "X_train= vectorizer.transform(sent_train) #transforming vectors\n",
    "X_test= vectorizer.transform(sent_test) #transforming vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6455203116304953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression() #Classifier\n",
    "classifier.fit(X_train, y_train) #Fitting model\n",
    "score = classifier.score(X_test, y_test) #Accuracy Score\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic tokenization was used and achieved accuracy of 64 percent. The two paths forward are as follow: make the model more complex or tokenize the words differently by using bigrams or lemmentization. Hopefully, by isolating the models and only tweaking one aspect of the data/moddel we will improve our accuracy.\n",
    "\n",
    "First I am going to try to change the tokenized inputs of the models:\n",
    "    1. Stemming and Lemmentization\n",
    "    2. Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmentization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>Change</th>\n",
       "      <th>Coded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Trump Invites Top Lawmakers in Effort To End S...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Kim Jong Un Extends Peace Overture to U.S.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>American Detained in Russia Isn’t a Spy, Famil...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>The Money Managers to Watch in 2019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Investors Try Not to Panic Over Stock Volatility</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               Text  Change  \\\n",
       "0  2019-01-01  Trump Invites Top Lawmakers in Effort To End S...     1.0   \n",
       "1  2019-01-01         Kim Jong Un Extends Peace Overture to U.S.     1.0   \n",
       "2  2019-01-01  American Detained in Russia Isn’t a Spy, Famil...     1.0   \n",
       "3  2019-01-01                The Money Managers to Watch in 2019     1.0   \n",
       "4  2019-01-01   Investors Try Not to Panic Over Stock Volatility     1.0   \n",
       "\n",
       "   Coded  \n",
       "0    1.0  \n",
       "1    1.0  \n",
       "2    1.0  \n",
       "3    1.0  \n",
       "4    1.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #Loading original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #Creating the PorterStemmer\n",
    "stemmed_words= [stemmer.stem(token) for token in df_tokens] #Stemming the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('new', 2688),\n",
       " ('u.s.', 2493),\n",
       " ('trump', 2073),\n",
       " ('compani', 1346),\n",
       " ('million', 1261),\n",
       " ('year', 1141),\n",
       " ('presid', 1089),\n",
       " ('say', 1088),\n",
       " ('report', 1082),\n",
       " ('hous', 1069),\n",
       " ('stock', 1049),\n",
       " ('market', 1026),\n",
       " ('trade', 948),\n",
       " ('china', 941),\n",
       " ('plan', 904),\n",
       " ('deal', 861),\n",
       " ('home', 809),\n",
       " ('could', 727),\n",
       " ('week', 719),\n",
       " ('get', 703),\n",
       " ('busi', 688),\n",
       " ('make', 669),\n",
       " ('democrat', 659),\n",
       " ('billion', 658),\n",
       " ('state', 656),\n",
       " ('sale', 651),\n",
       " ('one', 649),\n",
       " ('two', 647),\n",
       " ('take', 638),\n",
       " ('first', 621),\n",
       " ('investor', 617),\n",
       " ('bank', 603),\n",
       " ('world', 597),\n",
       " ('rate', 592),\n",
       " ('face', 589),\n",
       " ('former', 589),\n",
       " ('price', 586),\n",
       " ('big', 578),\n",
       " ('feder', 573),\n",
       " ('fed', 570),\n",
       " ('offici', 546),\n",
       " ('sell', 546),\n",
       " ('york', 546),\n",
       " ('back', 545),\n",
       " ('govern', 537),\n",
       " ('cut', 536),\n",
       " ('execut', 533),\n",
       " ('like', 523),\n",
       " ('citi', 516),\n",
       " ('would', 512),\n",
       " ('peopl', 509),\n",
       " ('ceo', 508),\n",
       " ('u.', 506),\n",
       " ('help', 504),\n",
       " ('move', 503),\n",
       " ('chief', 501),\n",
       " ('buy', 498),\n",
       " ('ad', 497),\n",
       " ('growth', 492),\n",
       " ('fund', 482),\n",
       " ('set', 481),\n",
       " ('risk', 479),\n",
       " ('tech', 475),\n",
       " ('hit', 469),\n",
       " ('call', 469),\n",
       " ('cfo', 466),\n",
       " ('look', 466),\n",
       " ('today', 465),\n",
       " ('seek', 463),\n",
       " ('talk', 463),\n",
       " ('amazon', 461),\n",
       " ('global', 458),\n",
       " ('use', 453),\n",
       " ('facebook', 451),\n",
       " ('rise', 448),\n",
       " ('pay', 447),\n",
       " ('rule', 447),\n",
       " ('rais', 447),\n",
       " ('economi', 438),\n",
       " ('logist', 438),\n",
       " ('show', 436),\n",
       " ('time', 436),\n",
       " ('offer', 434),\n",
       " ('nation', 423),\n",
       " ('american', 422),\n",
       " ('next', 418),\n",
       " ('work', 418),\n",
       " ('record', 417),\n",
       " ('administr', 415),\n",
       " ('morn', 415),\n",
       " ('investig', 414),\n",
       " ('court', 409),\n",
       " ('leader', 409),\n",
       " ('end', 407),\n",
       " ('charg', 406),\n",
       " ('chang', 406),\n",
       " ('month', 404),\n",
       " ('case', 404),\n",
       " ('may', 400),\n",
       " ('list', 400),\n",
       " ('expect', 400),\n",
       " ('firm', 399),\n",
       " ('job', 399),\n",
       " ('california', 398),\n",
       " ('power', 395),\n",
       " ('tax', 391),\n",
       " ('countri', 388),\n",
       " ('last', 387),\n",
       " ('share', 383),\n",
       " ('amid', 380),\n",
       " ('appl', 380),\n",
       " ('top', 379),\n",
       " ('push', 379),\n",
       " ('group', 379),\n",
       " ('come', 370),\n",
       " ('invest', 368),\n",
       " ('want', 365),\n",
       " ('fall', 363),\n",
       " ('financi', 361),\n",
       " ('oil', 354),\n",
       " ('way', 352),\n",
       " ('find', 350),\n",
       " ('depart', 345),\n",
       " ('women', 343),\n",
       " ('day', 341),\n",
       " ('probe', 339),\n",
       " ('challeng', 338),\n",
       " ('leav', 338),\n",
       " ('white', 336),\n",
       " ('ask', 333),\n",
       " ('data', 332),\n",
       " ('regul', 323),\n",
       " ('profit', 322),\n",
       " ('polit', 322),\n",
       " ('return', 321),\n",
       " ('maker', 321),\n",
       " ('design', 319),\n",
       " ('effort', 314),\n",
       " ('manag', 314),\n",
       " ('iran', 314),\n",
       " ('turn', 312),\n",
       " ('giant', 311),\n",
       " ('industri', 311),\n",
       " ('gener', 310),\n",
       " ('test', 307),\n",
       " ('earn', 306),\n",
       " ('target', 305),\n",
       " ('open', 302),\n",
       " ('econom', 302),\n",
       " ('offic', 302),\n",
       " ('public', 302),\n",
       " ('protest', 298),\n",
       " ('tariff', 297),\n",
       " ('tri', 296),\n",
       " ('wall', 295),\n",
       " ('ship', 295),\n",
       " ('street', 295),\n",
       " ('servic', 295),\n",
       " ('attack', 294),\n",
       " ('technolog', 293),\n",
       " ('north', 293),\n",
       " ('kong', 293),\n",
       " ('hong', 290),\n",
       " ('go', 289),\n",
       " ('major', 287),\n",
       " ('secur', 287),\n",
       " ('chines', 287),\n",
       " ('ahead', 286),\n",
       " ('fire', 286),\n",
       " ('brexit', 285),\n",
       " ('need', 283),\n",
       " ('boe', 283),\n",
       " ('start', 278),\n",
       " ('money', 276),\n",
       " ('fight', 276),\n",
       " ('border', 276),\n",
       " ('unit', 275),\n",
       " ('boost', 274),\n",
       " ('head', 273),\n",
       " ('program', 273),\n",
       " ('elect', 273),\n",
       " ('meet', 272),\n",
       " ('put', 271),\n",
       " ('keep', 270),\n",
       " ('spend', 270),\n",
       " ('sanction', 270),\n",
       " ('high', 269),\n",
       " ('health', 269),\n",
       " ('america', 268),\n",
       " ('grow', 267),\n",
       " ('question', 266),\n",
       " ('build', 265),\n",
       " ('name', 265),\n",
       " ('slow', 264),\n",
       " ('good', 264),\n",
       " ('war', 264),\n",
       " ('even', 264),\n",
       " ('colleg', 263),\n",
       " ('cost', 261),\n",
       " ('alleg', 261),\n",
       " ('brand', 260),\n",
       " ('win', 259),\n",
       " ('latest', 259),\n",
       " ('kill', 258),\n",
       " ('forc', 257),\n",
       " ('product', 257),\n",
       " ('worker', 256),\n",
       " ('lawmak', 251),\n",
       " ('biggest', 251),\n",
       " ('mani', 251),\n",
       " ('warn', 251),\n",
       " ('still', 251),\n",
       " ('vote', 251),\n",
       " ('team', 251),\n",
       " ('close', 250),\n",
       " ('futur', 250),\n",
       " ('famili', 249),\n",
       " ('debat', 248),\n",
       " ('bill', 248),\n",
       " ('googl', 248),\n",
       " ('great', 247),\n",
       " ('ipo', 247),\n",
       " ('campaign', 247),\n",
       " ('concern', 245),\n",
       " ('law', 244),\n",
       " ('step', 244),\n",
       " ('pressur', 243),\n",
       " ('2020', 243),\n",
       " ('includ', 243),\n",
       " ('u.k.', 243),\n",
       " ('order', 242),\n",
       " ('consum', 241),\n",
       " ('sign', 239),\n",
       " ('revenu', 239),\n",
       " ('polici', 239),\n",
       " ('part', 239),\n",
       " ('best', 238),\n",
       " ('demand', 237),\n",
       " ('tesla', 237),\n",
       " ('long', 237),\n",
       " ('run', 237),\n",
       " ('hire', 235),\n",
       " ('max', 235),\n",
       " ('second', 233),\n",
       " ('accord', 233),\n",
       " ('bond', 232),\n",
       " ('see', 231),\n",
       " ('2019', 230),\n",
       " ('saudi', 230),\n",
       " ('launch', 228),\n",
       " ('propos', 228),\n",
       " ('reach', 226),\n",
       " ('retir', 226),\n",
       " ('die', 224),\n",
       " ('digit', 223),\n",
       " ('three', 223),\n",
       " ('central', 221),\n",
       " ('support', 221),\n",
       " ('judg', 220),\n",
       " ('polic', 220),\n",
       " ('continu', 219),\n",
       " ('oper', 218),\n",
       " ('russia', 217),\n",
       " ('give', 217),\n",
       " ('gain', 216),\n",
       " ('luxuri', 216),\n",
       " ('ai', 216),\n",
       " ('car', 216),\n",
       " ('militari', 214),\n",
       " ('analyst', 214),\n",
       " ('huawei', 214),\n",
       " ('accus', 213),\n",
       " ('agre', 212),\n",
       " ('europ', 211),\n",
       " ('nearli', 210),\n",
       " ('custom', 210),\n",
       " ('follow', 209),\n",
       " ('estat', 209),\n",
       " ('senat', 209),\n",
       " ('drop', 208),\n",
       " ('issu', 208),\n",
       " ('mueller', 208),\n",
       " ('bid', 207),\n",
       " ('uber', 207),\n",
       " ('thing', 206),\n",
       " ('auction', 205),\n",
       " ('rival', 205),\n",
       " ('despit', 205),\n",
       " ('presidenti', 204),\n",
       " ('becom', 204),\n",
       " ('sinc', 203),\n",
       " ('school', 203),\n",
       " ('drug', 202),\n",
       " ('made', 202),\n",
       " ('hold', 201),\n",
       " ('race', 201),\n",
       " ('struggl', 201),\n",
       " ('system', 200),\n",
       " ('european', 200),\n",
       " ('behind', 200),\n",
       " ('role', 200),\n",
       " ('media', 199),\n",
       " ('live', 198),\n",
       " ('anoth', 198),\n",
       " ('author', 198),\n",
       " ('emerg', 198),\n",
       " ('potenti', 197),\n",
       " ('star', 197),\n",
       " ('increas', 197),\n",
       " ('game', 196),\n",
       " ('shift', 196),\n",
       " ('creat', 196),\n",
       " ('post', 196),\n",
       " ('aim', 195),\n",
       " ('develop', 195),\n",
       " ('ban', 195),\n",
       " ('shoot', 194),\n",
       " ('right', 194),\n",
       " ('account', 194),\n",
       " ('bet', 193),\n",
       " ('foreign', 193),\n",
       " ('play', 193),\n",
       " ('retail', 192),\n",
       " ('onlin', 192),\n",
       " ('paid', 192),\n",
       " ('737', 191),\n",
       " ('justic', 190),\n",
       " ('financ', 190),\n",
       " ('final', 189),\n",
       " ('prime', 189),\n",
       " ('minist', 189),\n",
       " ('startup', 187),\n",
       " ('problem', 187),\n",
       " ('capit', 186),\n",
       " ('critic', 186),\n",
       " ('agenc', 186),\n",
       " ('shutdown', 185),\n",
       " ('review', 184),\n",
       " ('board', 184),\n",
       " ('quarter', 184),\n",
       " ('intern', 184),\n",
       " ('strong', 184),\n",
       " ('london', 183),\n",
       " ('lead', 183),\n",
       " ('higher', 183),\n",
       " ('crisi', 183),\n",
       " ('washington', 182),\n",
       " ('battl', 181),\n",
       " ('recent', 181),\n",
       " ('tension', 181),\n",
       " ('scandal', 180),\n",
       " ('save', 180),\n",
       " ('korea', 179),\n",
       " ('biden', 179),\n",
       " ('threat', 179),\n",
       " ('owner', 178),\n",
       " ('life', 178),\n",
       " ('hope', 178),\n",
       " ('around', 178),\n",
       " ('debt', 176),\n",
       " ('claim', 176),\n",
       " ('decad', 176),\n",
       " ('russian', 176),\n",
       " ('food', 176),\n",
       " ('know', 175),\n",
       " ('declin', 175),\n",
       " ('union', 175),\n",
       " ('bring', 175),\n",
       " ('without', 175),\n",
       " ('control', 175),\n",
       " ('escap', 174),\n",
       " ('travel', 174),\n",
       " ('point', 174),\n",
       " ('strike', 174),\n",
       " ('releas', 174),\n",
       " ('expand', 174),\n",
       " ('news', 172),\n",
       " ('block', 172),\n",
       " ('lower', 172),\n",
       " ('parti', 172),\n",
       " ('remain', 172),\n",
       " ('pass', 172),\n",
       " ('insid', 171),\n",
       " ('much', 171),\n",
       " ('mark', 171),\n",
       " ('tv', 169),\n",
       " ('download', 168),\n",
       " ('mansion', 168),\n",
       " ('fashion', 167),\n",
       " ('10', 167),\n",
       " ('mexico', 167),\n",
       " ('allow', 167),\n",
       " ('found', 167),\n",
       " ('watch', 166),\n",
       " ('consid', 166),\n",
       " ('file', 166),\n",
       " ('prepar', 166),\n",
       " ('line', 165),\n",
       " ('dollar', 165),\n",
       " ('surg', 165),\n",
       " ('employe', 164),\n",
       " ('delay', 163),\n",
       " ('impeach', 163),\n",
       " ('summer', 162),\n",
       " ('privat', 162),\n",
       " ('resign', 162),\n",
       " ('venezuela', 162),\n",
       " ('nfl', 161),\n",
       " ('interest', 161),\n",
       " ('app', 161),\n",
       " ('british', 161),\n",
       " ('parent', 161),\n",
       " ('sec', 160),\n",
       " ('men', 160),\n",
       " ('pick', 160),\n",
       " ('nuclear', 159),\n",
       " ('largest', 159),\n",
       " ('result', 159),\n",
       " ('four', 158),\n",
       " ('valu', 158),\n",
       " ('john', 158),\n",
       " ('near', 157),\n",
       " ('earli', 157),\n",
       " ('candid', 157),\n",
       " ('whether', 157),\n",
       " ('cash', 156),\n",
       " ('tie', 156),\n",
       " ('stake', 155),\n",
       " ('nba', 155),\n",
       " ('suprem', 155),\n",
       " ('opinion', 155),\n",
       " ('fail', 154),\n",
       " ('south', 153),\n",
       " ('possibl', 153),\n",
       " ('secret', 153),\n",
       " ('congress', 153),\n",
       " ('less', 153),\n",
       " ('prosecutor', 153),\n",
       " ('age', 153),\n",
       " ('worri', 153),\n",
       " ('stop', 152),\n",
       " ('threaten', 152),\n",
       " ('store', 152),\n",
       " ('suppli', 152),\n",
       " ('act', 151),\n",
       " ('man', 151),\n",
       " ('art', 151),\n",
       " ('decis', 151),\n",
       " ('republican', 151),\n",
       " ('lose', 151),\n",
       " ('u.k', 151),\n",
       " ('low', 151),\n",
       " ('lawsuit', 151),\n",
       " ('photo', 150),\n",
       " ('social', 150),\n",
       " ('2018', 149),\n",
       " ('toward', 149),\n",
       " ('strategi', 149),\n",
       " ('defend', 149),\n",
       " ('stay', 148),\n",
       " ('five', 148),\n",
       " ('break', 148),\n",
       " ('key', 148),\n",
       " ('auto', 148),\n",
       " ('signal', 147),\n",
       " ('weigh', 146),\n",
       " ('loss', 146),\n",
       " ('insur', 146),\n",
       " ('lawyer', 146),\n",
       " ('corpor', 145),\n",
       " ('chairman', 145),\n",
       " ('well', 145),\n",
       " ('import', 145),\n",
       " ('johnson', 145),\n",
       " ('join', 144),\n",
       " ('better', 144),\n",
       " ('death', 144),\n",
       " ('level', 144),\n",
       " ('airlin', 144),\n",
       " ('attorney', 144),\n",
       " ('least', 144),\n",
       " ('small', 143),\n",
       " ('real', 143),\n",
       " ('asset', 143),\n",
       " ('beyond', 143),\n",
       " ('drive', 142),\n",
       " ('legal', 142),\n",
       " ('across', 141),\n",
       " ('arrest', 141),\n",
       " ('among', 141),\n",
       " ('ralli', 140),\n",
       " ('fear', 140),\n",
       " ('person', 140),\n",
       " ('cloud', 140),\n",
       " ('manhattan', 139),\n",
       " ('deliveri', 139),\n",
       " ('outlook', 139),\n",
       " ('soccer', 139),\n",
       " ('crash', 138),\n",
       " ('provid', 138),\n",
       " ('network', 138),\n",
       " ('cmo', 137),\n",
       " ('buyer', 137),\n",
       " ('immigr', 137),\n",
       " ('special', 137),\n",
       " ('limit', 137),\n",
       " ('fine', 137),\n",
       " ('truck', 136),\n",
       " ('wine', 136),\n",
       " ('aid', 136),\n",
       " ('netflix', 135),\n",
       " ('scrutini', 134),\n",
       " ('cio', 133),\n",
       " ('20', 133),\n",
       " ('syria', 133),\n",
       " ('robert', 133),\n",
       " ('reserv', 133),\n",
       " ('current', 132),\n",
       " ('histori', 132),\n",
       " ('merger', 132),\n",
       " ('disput', 131),\n",
       " ('advis', 131),\n",
       " ('treasuri', 131),\n",
       " ('season', 130),\n",
       " ('student', 130),\n",
       " ('activ', 130),\n",
       " ('tell', 130),\n",
       " ('labor', 129),\n",
       " ('defens', 129),\n",
       " ('advertis', 129),\n",
       " ('dow', 129),\n",
       " ('number', 129),\n",
       " ('also', 129),\n",
       " ('guid', 128),\n",
       " ('respons', 128),\n",
       " ('goe', 128),\n",
       " ('properti', 128),\n",
       " ('ukrain', 128),\n",
       " ('agreement', 128),\n",
       " ('audit', 128),\n",
       " ('might', 127),\n",
       " ('negoti', 127),\n",
       " ('stream', 127),\n",
       " ('prison', 127),\n",
       " ('payment', 127),\n",
       " ('11', 126),\n",
       " ('miss', 126),\n",
       " ('search', 126),\n",
       " ('requir', 126),\n",
       " ('warren', 125),\n",
       " ('produc', 125),\n",
       " ('walmart', 125),\n",
       " ('collect', 124),\n",
       " ('begin', 124),\n",
       " ('approv', 124),\n",
       " ('discuss', 124),\n",
       " ('center', 124),\n",
       " ('pull', 123),\n",
       " ('fuel', 123),\n",
       " ('corrupt', 123),\n",
       " ('region', 123),\n",
       " ('committe', 123),\n",
       " ('minut', 122),\n",
       " ('spark', 122),\n",
       " ('learn', 122),\n",
       " ('robot', 122),\n",
       " ('chain', 122),\n",
       " ('protect', 121),\n",
       " ('japan', 121),\n",
       " ('freight', 121),\n",
       " ('billionair', 120),\n",
       " ('mean', 120),\n",
       " ('free', 119),\n",
       " ('india', 118),\n",
       " ('avoid', 118),\n",
       " ('e', 118),\n",
       " ('commun', 118),\n",
       " ('eas', 118),\n",
       " ('energi', 117),\n",
       " ('50', 117),\n",
       " ('abus', 117),\n",
       " ('french', 117),\n",
       " ('angel', 117),\n",
       " ('larg', 117),\n",
       " ('eu', 116),\n",
       " ('measur', 116),\n",
       " ('pg', 116),\n",
       " ('children', 115),\n",
       " ('guilti', 115),\n",
       " ('left', 115),\n",
       " ('draw', 114),\n",
       " ('opposit', 114),\n",
       " ('promis', 113),\n",
       " ('player', 113),\n",
       " ('trial', 113),\n",
       " ('electr', 113),\n",
       " ('dead', 113),\n",
       " ('beij', 113),\n",
       " ('intellig', 113),\n",
       " ('sever', 113),\n",
       " ('briberi', 113),\n",
       " ('fix', 112),\n",
       " ('overhaul', 112),\n",
       " ('press', 112),\n",
       " ('super', 112),\n",
       " ('deliv', 111),\n",
       " ('model', 111),\n",
       " ('texa', 111),\n",
       " ('air', 111),\n",
       " ('lo', 111),\n",
       " ('5g', 111),\n",
       " ('settlement', 111),\n",
       " ('loan', 111),\n",
       " ('past', 110),\n",
       " ('thousand', 110),\n",
       " ('soon', 110),\n",
       " ('disney', 110),\n",
       " ('fraud', 110),\n",
       " ('alli', 110),\n",
       " ('secretari', 109),\n",
       " ('inflat', 109),\n",
       " ('video', 109),\n",
       " ('beach', 109),\n",
       " ('budget', 109),\n",
       " ('manufactur', 108),\n",
       " ('track', 108),\n",
       " ('west', 108),\n",
       " ('cup', 108),\n",
       " ('care', 108),\n",
       " ('advanc', 108),\n",
       " ('send', 108),\n",
       " ('violat', 108),\n",
       " ('black', 107),\n",
       " ('half', 107),\n",
       " ('fell', 107),\n",
       " ('san', 107),\n",
       " ('think', 106),\n",
       " ('sport', 106),\n",
       " ('celebr', 106),\n",
       " ('bankruptci', 106),\n",
       " ('side', 106),\n",
       " ('fresh', 106),\n",
       " ('away', 106),\n",
       " ('book', 105),\n",
       " ('took', 105),\n",
       " ('land', 105),\n",
       " ('ago', 105),\n",
       " ('seri', 105),\n",
       " ('—', 105),\n",
       " ('exit', 105),\n",
       " ('space', 104),\n",
       " ('project', 104),\n",
       " ('relat', 104),\n",
       " ('soar', 104),\n",
       " ('franc', 104),\n",
       " ('barr', 104),\n",
       " ('organ', 104),\n",
       " ('light', 104),\n",
       " ('subpoena', 104),\n",
       " ('extend', 103),\n",
       " ('beat', 103),\n",
       " ('opioid', 103),\n",
       " ('initi', 103),\n",
       " ('turkey', 103),\n",
       " ('wework', 103),\n",
       " ('doctor', 103),\n",
       " ('basebal', 103),\n",
       " ('crimin', 103),\n",
       " ('far', 102),\n",
       " ('clash', 102),\n",
       " ('microsoft', 101),\n",
       " ('place', 101),\n",
       " ('train', 101),\n",
       " ('march', 101),\n",
       " ('joe', 101),\n",
       " ('migrant', 101),\n",
       " ('standard', 101),\n",
       " ('enough', 101),\n",
       " ('director', 101),\n",
       " ('matter', 101),\n",
       " ('brazil', 100),\n",
       " ('us', 100),\n",
       " ('music', 100),\n",
       " ('plead', 100),\n",
       " ('phone', 100),\n",
       " ('approach', 100),\n",
       " ('worth', 99),\n",
       " ('ga', 99),\n",
       " ('ever', 99),\n",
       " ('legisl', 99),\n",
       " ('partner', 99),\n",
       " ('term', 99),\n",
       " ('ground', 99),\n",
       " ('yield', 99),\n",
       " ('old', 99),\n",
       " ('factori', 99),\n",
       " ('safeti', 99),\n",
       " ('member', 99),\n",
       " ('penthous', 99),\n",
       " ('bowl', 99),\n",
       " ('green', 99),\n",
       " ('at', 99),\n",
       " ('climat', 99),\n",
       " ('settl', 98),\n",
       " ('add', 98),\n",
       " ('florida', 98),\n",
       " ('caus', 98),\n",
       " ('arabia', 98),\n",
       " ('powel', 98),\n",
       " ('islam', 98),\n",
       " ('benefit', 98),\n",
       " ('nike', 98),\n",
       " ('antitrust', 98),\n",
       " ('gun', 97),\n",
       " ('15', 97),\n",
       " ('500', 97),\n",
       " ('lost', 97),\n",
       " ('differ', 97),\n",
       " ('tanker', 97),\n",
       " ('lyft', 97),\n",
       " ('southern', 96),\n",
       " ('rare', 96),\n",
       " ('collaps', 96),\n",
       " ('user', 96),\n",
       " ('iphon', 96),\n",
       " ('announc', 96),\n",
       " ('hotel', 96),\n",
       " ('readi', 96),\n",
       " ('michael', 96),\n",
       " ('trip', 96),\n",
       " ('rose', 96),\n",
       " ('tap', 96),\n",
       " ('studi', 96),\n",
       " ('littl', 95),\n",
       " ('hour', 95),\n",
       " ('water', 95),\n",
       " ('urg', 95),\n",
       " ('hear', 95),\n",
       " ('let', 95),\n",
       " ('appear', 95),\n",
       " ('francisco', 95),\n",
       " ('love', 95),\n",
       " ('stori', 94),\n",
       " ('focu', 94),\n",
       " ('surpris', 94),\n",
       " ('sue', 94),\n",
       " ('expens', 94),\n",
       " ('jet', 94),\n",
       " ('venezuelan', 94),\n",
       " ('lift', 94),\n",
       " ('influenc', 94),\n",
       " ('option', 94),\n",
       " ('activist', 93),\n",
       " ('local', 93),\n",
       " ('conflict', 93),\n",
       " ('suggest', 93),\n",
       " ('research', 93),\n",
       " ('israel', 93),\n",
       " ('island', 93),\n",
       " ('abort', 93),\n",
       " ('enter', 92),\n",
       " ('gm', 92),\n",
       " ('privaci', 92),\n",
       " ('read', 92),\n",
       " ('founder', 92),\n",
       " ('dam', 92),\n",
       " ('late', 91),\n",
       " ('hand', 91),\n",
       " ('contain', 91),\n",
       " ('flood', 91),\n",
       " ('process', 91),\n",
       " ('histor', 91),\n",
       " ('william', 91),\n",
       " ('road', 91),\n",
       " ('competit', 91),\n",
       " ('area', 91),\n",
       " ('contract', 90),\n",
       " ('100', 90),\n",
       " ('newslett', 90),\n",
       " ('deadli', 90),\n",
       " ('p', 90),\n",
       " ('governor', 90),\n",
       " ('sentenc', 90),\n",
       " ('led', 90),\n",
       " ('site', 89),\n",
       " ('field', 89),\n",
       " ('attempt', 89),\n",
       " ('detail', 89),\n",
       " ('apart', 89),\n",
       " ('complianc', 89),\n",
       " ('six', 89),\n",
       " ('wsj', 89),\n",
       " ('pro', 89),\n",
       " ('valley', 89),\n",
       " ('stage', 89),\n",
       " ('unveil', 89),\n",
       " ('improv', 88),\n",
       " ('jump', 88),\n",
       " ('modern', 88),\n",
       " ('stand', 88),\n",
       " ('sunday', 88),\n",
       " ('mortgag', 88),\n",
       " ('impact', 88),\n",
       " ('messag', 88),\n",
       " ('town', 88),\n",
       " ('appeal', 87),\n",
       " ('spot', 87),\n",
       " ('credit', 86),\n",
       " ('middl', 86),\n",
       " ('increasingli', 86),\n",
       " ('inform', 86),\n",
       " ('coach', 86),\n",
       " ('human', 86),\n",
       " ('young', 86),\n",
       " ('congression', 86),\n",
       " ('friday', 86),\n",
       " ('hollywood', 86),\n",
       " ('dozen', 86),\n",
       " ('restrict', 85),\n",
       " ('bar', 85),\n",
       " ('feel', 85),\n",
       " ('internet', 85),\n",
       " ('view', 85),\n",
       " ('climb', 85),\n",
       " ('twitter', 85),\n",
       " ('canada', 85),\n",
       " ('chip', 85),\n",
       " ('deutsch', 85),\n",
       " ('virginia', 85),\n",
       " ('becam', 84),\n",
       " ('favorit', 84),\n",
       " ('forecast', 84),\n",
       " ('wildfir', 84),\n",
       " ('later', 84),\n",
       " ('suit', 84),\n",
       " ('other', 84),\n",
       " ('expans', 84),\n",
       " ('hot', 84),\n",
       " ('clear', 84),\n",
       " ('built', 84),\n",
       " ('success', 83),\n",
       " ('troubl', 83),\n",
       " ('u.s.-china', 83),\n",
       " ('leadership', 83),\n",
       " ('domin', 83),\n",
       " ('gop', 83),\n",
       " ('iranian', 83),\n",
       " ('counsel', 83),\n",
       " ('juli', 83),\n",
       " ('boom', 82),\n",
       " ('short', 82),\n",
       " ('loom', 82),\n",
       " ('bad', 82),\n",
       " ('third', 82),\n",
       " ('action', 82),\n",
       " ('document', 82),\n",
       " ('it', 81),\n",
       " ('popular', 81),\n",
       " ('slowdown', 81),\n",
       " ('trend', 81),\n",
       " ('progress', 81),\n",
       " ('purchas', 81),\n",
       " ('spread', 81),\n",
       " ('panel', 81),\n",
       " ('june', 81),\n",
       " ('carrier', 80),\n",
       " ('hundr', 80),\n",
       " ('access', 80),\n",
       " ('guidanc', 80),\n",
       " ('staff', 80),\n",
       " ('yet', 80),\n",
       " ('sexual', 80),\n",
       " ('withdraw', 80),\n",
       " ('posit', 80),\n",
       " ('outsid', 80),\n",
       " ('card', 79),\n",
       " ('sector', 79),\n",
       " ('friend', 79),\n",
       " ('answer', 79),\n",
       " ('prospect', 79),\n",
       " ('red', 79),\n",
       " ('uncertainti', 79),\n",
       " ('escal', 79),\n",
       " ('mr.', 79),\n",
       " ('hill', 79),\n",
       " ('monday', 78),\n",
       " ('mass', 78),\n",
       " ('disrupt', 78),\n",
       " ('softwar', 78),\n",
       " ('rent', 78),\n",
       " ('epstein', 78),\n",
       " ('happen', 77),\n",
       " ('request', 77),\n",
       " ('exchang', 77),\n",
       " ('flight', 77),\n",
       " ('shop', 77),\n",
       " ('voter', 77),\n",
       " ('weak', 77),\n",
       " ('longer', 77),\n",
       " ('weekend', 77),\n",
       " ('suspect', 77),\n",
       " ('path', 76),\n",
       " ('expert', 76),\n",
       " ('employ', 76),\n",
       " ('univers', 76),\n",
       " ('hedg', 76),\n",
       " ('pelosi', 76),\n",
       " ('england', 76),\n",
       " ('platform', 76),\n",
       " ('divid', 76),\n",
       " ('reduc', 76),\n",
       " ('practic', 76),\n",
       " ('content', 76),\n",
       " ('often', 76),\n",
       " ('cohen', 76),\n",
       " ('wave', 76),\n",
       " ('familiar', 76),\n",
       " ('cool', 75),\n",
       " ('chicago', 75),\n",
       " ('cultur', 75),\n",
       " ('almost', 75),\n",
       " ('fan', 75),\n",
       " ('crime', 75),\n",
       " ('comput', 75),\n",
       " ('ford', 75),\n",
       " ('edg', 74),\n",
       " ('musk', 74),\n",
       " ('involv', 74),\n",
       " ('reject', 74),\n",
       " ('reviv', 74),\n",
       " ('reveal', 74),\n",
       " ('combin', 74),\n",
       " ('leagu', 74),\n",
       " ('prompt', 74),\n",
       " ('recess', 74),\n",
       " ('movi', 74),\n",
       " ('summit', 74),\n",
       " ('wrong', 74),\n",
       " ('woman', 74),\n",
       " ('declar', 74),\n",
       " ('economist', 74),\n",
       " ('bigger', 74),\n",
       " ('whistleblow', 74),\n",
       " ('trucker', 74),\n",
       " ('volatil', 73),\n",
       " ('effect', 73),\n",
       " ('held', 73),\n",
       " ('handl', 73),\n",
       " ('got', 73),\n",
       " ('hard', 73),\n",
       " ('packag', 73),\n",
       " ('enforc', 73),\n",
       " ('senior', 73),\n",
       " ('wait', 73),\n",
       " ('netanyahu', 73),\n",
       " ('coast', 73),\n",
       " ('export', 72),\n",
       " ('troop', 72),\n",
       " ('everi', 72),\n",
       " ('highlight', 72),\n",
       " ('known', 72),\n",
       " ('journal', 72),\n",
       " ('storm', 72),\n",
       " ('realli', 72),\n",
       " ('2016', 72),\n",
       " ('speed', 72),\n",
       " ('silicon', 72),\n",
       " ('sustain', 72),\n",
       " ('reason', 72),\n",
       " ('coupl', 72),\n",
       " ('must', 72),\n",
       " ('east', 71),\n",
       " ('prevent', 71),\n",
       " ('resolv', 71),\n",
       " ('visit', 71),\n",
       " ('plane', 71),\n",
       " ('fbi', 71),\n",
       " ('asylum', 71),\n",
       " ('associ', 71),\n",
       " ('perform', 71),\n",
       " ('unlik', 71),\n",
       " ('idea', 70),\n",
       " ('gold', 70),\n",
       " ('wear', 70),\n",
       " ('eye', 70),\n",
       " ('incom', 70),\n",
       " ('danger', 70),\n",
       " ('never', 70),\n",
       " ('acquisit', 70),\n",
       " ('victim', 70),\n",
       " ('seiz', 70),\n",
       " ('sought', 70),\n",
       " ('debut', 70),\n",
       " ('address', 69),\n",
       " ('gap', 69),\n",
       " ('wage', 69),\n",
       " ('autom', 69),\n",
       " ('tough', 69),\n",
       " ('kim', 68),\n",
       " ('decid', 68),\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(stemmed_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There really is not a difference between the stemmed words vs. the traditional tokenized word counts. Now lets run a model to see if this had any effect on the modeling accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input): #Defining a function to be able to load this into our model as the tokenizer\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [PorterStemmer().stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text']\n",
    "y=df['Coded']\n",
    "\n",
    "sent_train, sent_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=1)\n",
    "\n",
    "vectorizer= CountVectorizer(tokenizer=stemming_tokenizer) #Everything else is the same as passed models, except tweaking tokenizer\n",
    "vectorizer.fit(sent_train)\n",
    "\n",
    "X_train= vectorizer.transform(sent_train)\n",
    "X_test= vectorizer.transform(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.92 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", round(score*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, stemming did not improve our model by very much. Essentially it improved by a little under a percentage point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeration(x):\n",
    "    for x in df_tokens:\n",
    "        word= []\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        word.append(lemmatizer.lemmatize(x))\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text']\n",
    "y=df['Coded']\n",
    "\n",
    "sent_train, sent_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=1)\n",
    "\n",
    "vectorizer= CountVectorizer(tokenizer=lemmatizeration)\n",
    "vectorizer.fit(sent_train)\n",
    "\n",
    "X_train= vectorizer.transform(sent_train)\n",
    "X_test= vectorizer.transform(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.93 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", round(score*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing the text gave us a identical result to stemming. I am not confident that changing the mode of text will help the model, but I will try one last strategy, Bigrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy for tokenizing is using bigrams, which are words that essentailly belong or show up together in sentances. The computer is able to recongnize which words belong together and which ones do not. Below is the analysis of the bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('president', 'trump'), 0.0017477505802716874),\n",
       " (('new', 'york'), 0.0016830190772986618),\n",
       " (('logistics', 'report'), 0.0009216533042349815),\n",
       " (('today', 'logistics'), 0.0009216533042349815),\n",
       " (('trump', 'administration'), 0.0009154883991899314),\n",
       " (('hong', 'kong'), 0.0008939112315322563),\n",
       " (('white', 'house'), 0.0008939112315322563),\n",
       " (('wall', 'street'), 0.0006473150297302546),\n",
       " (('morning', 'risk'), 0.0006010782418923793),\n",
       " (('risk', 'report'), 0.0006010782418923793),\n",
       " (('737', 'max'), 0.0005579239065770289),\n",
       " (('morning', 'download'), 0.0004901099510814785),\n",
       " (('york', 'city'), 0.0004746976884688534),\n",
       " (('supreme', 'court'), 0.0004407907107210781),\n",
       " (('prime', 'minister'), 0.0004315433531535031),\n",
       " (('great', 'escapes'), 0.000422295995585928),\n",
       " (('justice', 'department'), 0.00040380128045077785),\n",
       " (('last', 'year'), 0.0003791416602705777),\n",
       " (('north', 'korea'), 0.0003760592077480527),\n",
       " (('paid', 'program'), 0.00037297675522552764),\n",
       " (('pg', 'e'), 0.0003575644926129025),\n",
       " (('federal', 'reserve'), 0.0003390697774777524),\n",
       " (('chief', 'executive'), 0.0003359873249552274),\n",
       " (('los', 'angeles'), 0.00032982241991017734),\n",
       " (('saudi', 'arabia'), 0.0003020803472074521),\n",
       " (('attorney', 'general'), 0.00029591544216240207),\n",
       " (('analyst', 'says'), 0.0002928329896398771),\n",
       " (('san', 'francisco'), 0.0002928329896398771),\n",
       " (('world', 'cup'), 0.00028975053711735203),\n",
       " (('cmo', 'today'), 0.000283585632072302),\n",
       " (('trade', 'talks'), 0.000283585632072302),\n",
       " (('super', 'bowl'), 0.000280503179549777),\n",
       " (('interest', 'rates'), 0.00027742072702725194),\n",
       " (('today', 'newsletter'), 0.00027433827450472695),\n",
       " (('islamic', 'state'), 0.0002681733694596769),\n",
       " (('week', 'ahead'), 0.0002681733694596769),\n",
       " (('u.s.', 'stocks'), 0.00026200846441462686),\n",
       " (('economy', 'week'), 0.0002589260118921018),\n",
       " (('new', 'cfo'), 0.00025276110684705177),\n",
       " (('real', 'estate'), 0.00025276110684705177),\n",
       " (('rate', 'cut'), 0.0002496786543245268),\n",
       " (('finance', 'chief'), 0.0002465962018020017),\n",
       " (('central', 'bank'), 0.0002404312967569517),\n",
       " (('deutsche', 'bank'), 0.0002404312967569517),\n",
       " (('mueller', 'report'), 0.0002404312967569517),\n",
       " (('trade', 'deal'), 0.00023118393918937665),\n",
       " (('joe', 'biden'), 0.0002250190341443266),\n",
       " (('names', 'new'), 0.0002250190341443266),\n",
       " (('first', 'time'), 0.00022193658162180158),\n",
       " (('democratic', 'presidential'), 0.00021885412909927656)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures() #Bigrams created and scored\n",
    "wsj_finder = BigramCollocationFinder.from_words(df_tokens)\n",
    "wsj_scored = wsj_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "wsj_scored[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI score: measure of association used in information theory and statistics referring to single events.\n",
    "\n",
    "Below are the calculations and code for this. It is very interesting due to the fact that it can call out important events that have happened over the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('abdelaziz', 'bouteflika'), 15.985561800747938),\n",
       " (('clayton', 'kershaw'), 15.985561800747938),\n",
       " (('edith', 'wharton'), 15.985561800747938),\n",
       " (('f.h', 'bertling'), 15.985561800747938),\n",
       " (('fire-', 'paparazzi-proofed'), 15.985561800747938),\n",
       " (('harriet', 'tubman'), 15.985561800747938),\n",
       " (('history—and', 'odds—are'), 15.985561800747938),\n",
       " (('javad', 'zarif'), 15.985561800747938),\n",
       " (('johns', 'hopkins'), 15.985561800747938),\n",
       " (('karlie', 'kloss'), 15.985561800747938),\n",
       " (('kershaw', 'fastball'), 15.985561800747938),\n",
       " (('lil', 'nas'), 15.985561800747938),\n",
       " (('lópez', 'obrador'), 15.985561800747938),\n",
       " (('nipsey', 'hussle'), 15.985561800747938),\n",
       " (('questions—and', 'answers—coming'), 15.985561800747938),\n",
       " (('randall', 'stephenson'), 15.985561800747938),\n",
       " (('sous', 'vide'), 15.985561800747938),\n",
       " (('triple—or', 'better—in'), 15.985561800747938),\n",
       " (('abn', 'amro'), 15.722527394914142),\n",
       " (('achilles', 'tendon'), 15.722527394914142),\n",
       " (('andrés', 'manuel'), 15.722527394914142),\n",
       " (('b-12', 'deficiency'), 15.722527394914142),\n",
       " (('boe', 'carney'), 15.722527394914142),\n",
       " (('cma', 'cgm'), 15.722527394914142),\n",
       " (('demography', 'destiny'), 15.722527394914142),\n",
       " (('dennis', 'muilenburg'), 15.722527394914142),\n",
       " (('devour', 'advertises'), 15.722527394914142),\n",
       " (('downton', 'abbey'), 15.722527394914142),\n",
       " (('gisele', 'bündchen'), 15.722527394914142),\n",
       " (('manuel', 'lópez'), 15.722527394914142),\n",
       " (('olivia', 'newton-john'), 15.722527394914142),\n",
       " (('pluses—and', 'perils—of'), 15.722527394914142),\n",
       " (('suitcases', 'pallets'), 15.722527394914142),\n",
       " (('td', 'ameritrade'), 15.722527394914142),\n",
       " (('virgil', 'abloh'), 15.722527394914142),\n",
       " (('greubel', 'forsey'), 15.500134973577698),\n",
       " (('paydays', 'benihana'), 15.500134973577698),\n",
       " (('talon', '1000x'), 15.500134973577698),\n",
       " (('telefonica', 'brasil'), 15.500134973577698),\n",
       " (('bursting', 'seams'), 15.500134973577694),\n",
       " (('endorsing', 'corporate-led'), 15.500134973577694),\n",
       " (('forecasting', 'aflac'), 15.500134973577694),\n",
       " (('karl', 'lagerfeld'), 15.500134973577694),\n",
       " (('roe', 'v.'), 15.500134973577694),\n",
       " (('sharapova', 'loneliness'), 15.500134973577694),\n",
       " (('spotted', 'lanternfly'), 15.500134973577694),\n",
       " (('swim', 'trunks'), 15.500134973577694),\n",
       " (('tents', 'glamping'), 15.500134973577694),\n",
       " (('hotly', 'contested'), 15.45949298908035),\n",
       " (('muhammad', 'ali'), 15.45949298908035),\n",
       " (('3.8', 'million-year-old'), 15.3074898956353),\n",
       " (('abdel', 'fattah'), 15.3074898956353),\n",
       " (('bakr', 'al-baghdadi'), 15.3074898956353),\n",
       " (('ellen', 'degeneres'), 15.3074898956353),\n",
       " (('estée', 'lauder'), 15.3074898956353),\n",
       " (('gen', 'z'), 15.3074898956353),\n",
       " (('gilroy', 'garlic'), 15.3074898956353),\n",
       " (('spearhead', 'female-entrepreneurs'), 15.3074898956353),\n",
       " (('advertises', 'pornhub'), 15.307489895635296),\n",
       " (('costa', 'rica'), 15.307489895635296),\n",
       " (('evergreen', 'hapag-lloyd'), 15.307489895635296),\n",
       " (('k-pop', 'fest'), 15.307489895635296),\n",
       " (('mobs', 'invaded'), 15.307489895635296),\n",
       " (('seams', 'k-pop'), 15.307489895635296),\n",
       " (('decked', 'neon'), 15.277742552241246),\n",
       " (('v.', 'wade'), 15.277742552241246),\n",
       " (('cyril', 'ramaphosa'), 15.237100567743902),\n",
       " (('groove', 'reclaiming'), 15.237100567743902),\n",
       " (('brooks', 'koepka'), 15.137564894192987),\n",
       " (('capturing', 'cause-and-effect'), 15.137564894192987),\n",
       " (('citi', 'advertiser'), 15.137564894192987),\n",
       " (('compressed', 'margins'), 15.137564894192987),\n",
       " (('j.c.', 'penney'), 15.137564894192987),\n",
       " (('lady', 'gaga'), 15.137564894192987),\n",
       " (('mardi', 'gras'), 15.137564894192987),\n",
       " (('megan', 'rapinoe'), 15.137564894192987),\n",
       " (('patek', 'philippe'), 15.137564894192987),\n",
       " (('rob', 'gronkowski'), 15.137564894192987),\n",
       " (('sheldon', 'adelson'), 15.137564894192987),\n",
       " (('simone', 'biles'), 15.137564894192987),\n",
       " (('facetime', 'bug'), 15.137564894192984),\n",
       " (('jair', 'bolsonaro'), 15.137564894192984),\n",
       " (('vitamin', 'b-12'), 15.137564894192984),\n",
       " (('gavin', 'newsom'), 15.114844817692905),\n",
       " (('pitches', 'renewable'), 15.044455489801503),\n",
       " (('russell', 'westbrook'), 15.044455489801503),\n",
       " (('beach-ready', 'pasta'), 14.985561800747938),\n",
       " (('bny', 'mellon'), 14.985561800747938),\n",
       " (('embraces', 'cowtown'), 14.985561800747938),\n",
       " (('gilead', 'sciences'), 14.985561800747938),\n",
       " (('gwen', 'stefani'), 14.985561800747938),\n",
       " (('honda', 'talon'), 14.985561800747938),\n",
       " (('liv', 'tyler'), 14.985561800747938),\n",
       " (('maria', 'sharapova'), 14.985561800747938),\n",
       " (('mornington', 'peninsula'), 14.985561800747938),\n",
       " (('mustang', 'minivan'), 14.985561800747938),\n",
       " (('outdated', 'notions'), 14.985561800747938),\n",
       " (('spelling', 'bee'), 14.985561800747938),\n",
       " (('vimeo', 'chases'), 14.985561800747938),\n",
       " (('worst-case', 'scenario'), 14.985561800747938),\n",
       " (('yrc', 'teamsters'), 14.985561800747938),\n",
       " (('barnes', 'noble'), 14.985561800747936),\n",
       " (('franchising', 'donuts'), 14.985561800747936),\n",
       " (('shaquille', \"o'neal\"), 14.985561800747936),\n",
       " (('turks', 'caicos'), 14.985561800747936),\n",
       " (('automotive', 'downshift'), 14.985561800747934),\n",
       " (('cory', 'booker'), 14.985561800747934),\n",
       " (('foothold', 'u.s.-asia'), 14.985561800747934),\n",
       " (('matt', 'cohler'), 14.985561800747934),\n",
       " (('nick', 'saban'), 14.985561800747934),\n",
       " (('perhaps', 'exponential'), 14.985561800747934),\n",
       " (('trunks', 'shorts'), 14.985561800747934),\n",
       " (('lori', 'loughlin'), 14.915172472856536),\n",
       " (('moneyed', 'montauk'), 14.915172472856536),\n",
       " (('reporter', 'phablet'), 14.915172472856536),\n",
       " (('soliciting', 'prostitution'), 14.915172472856536),\n",
       " (('plain', 'butter'), 14.892452396356454),\n",
       " (('amro', 'ramps'), 14.874530488359191),\n",
       " (('jennifer', 'aniston'), 14.874530488359191),\n",
       " (('leslie', 'wexner'), 14.874530488359191),\n",
       " (('qualified', 'enthusiastic'), 14.874530488359191),\n",
       " (('skiing', 'uphill'), 14.874530488359191),\n",
       " (('kellyanne', 'conway'), 14.848058276998003),\n",
       " (('leonardo', 'dicaprio'), 14.848058276998003),\n",
       " (('cape', 'cod'), 14.848058276998001),\n",
       " (('f.', 'seib'), 14.848058276998001),\n",
       " (('phil', 'spector'), 14.848058276998001),\n",
       " (('satya', 'nadella'), 14.848058276998001),\n",
       " (('silos', 'fend'), 14.848058276998001),\n",
       " (('exxon', 'mobil'), 14.848058276998),\n",
       " (('recep', 'tayyip'), 14.848058276998),\n",
       " (('elaine', 'chao'), 14.822063068465056),\n",
       " (('invasive', 'spotted'), 14.822063068465056),\n",
       " (('apex', 'legends'), 14.792916722805543),\n",
       " (('scuba', 'tunnels'), 14.792916722805543),\n",
       " (('underground', 'scuba'), 14.792916722805543),\n",
       " (('brad', 'pitt'), 14.763169379411487),\n",
       " (('60s', '70s—and'), 14.722527394914145),\n",
       " (('abu', 'bakr'), 14.722527394914145),\n",
       " (('parker', 'liv'), 14.722527394914145),\n",
       " (('sizing', 'revolutionizing'), 14.722527394914145),\n",
       " (('abu', 'dhabi'), 14.722527394914142),\n",
       " (('aniston', 'brad'), 14.722527394914142),\n",
       " (('azz', 'replaces'), 14.722527394914142),\n",
       " (('circling', 'groupon'), 14.722527394914142),\n",
       " (('i.m', 'pei'), 14.722527394914142),\n",
       " (('pedigree', '22.5'), 14.722527394914142),\n",
       " (('shipment', 'windfall'), 14.722527394914142),\n",
       " (('strait', 'hormuz'), 14.722527394914142),\n",
       " (('gerald', 'f.'), 14.678133275555691),\n",
       " (('benny', 'gantz'), 14.629417990522661),\n",
       " (('c.', 'wang'), 14.629417990522661),\n",
       " (('elijah', 'cummings'), 14.629417990522661),\n",
       " (('jamie', 'dimon'), 14.629417990522661),\n",
       " (('lockdown', 'paralyzes'), 14.622991721363231),\n",
       " (('alnylam', 'pharmaceuticals'), 14.607050177494207),\n",
       " (('cause-and-effect', 'relationships'), 14.607050177494207),\n",
       " (('merrill', 'lynch'), 14.607050177494207),\n",
       " (('felicity', 'huffman'), 14.607050177494205),\n",
       " (('inclusive', 'sizing'), 14.607050177494205),\n",
       " (('lloyd', 'wright'), 14.607050177494205),\n",
       " (('meredith', 'vieira'), 14.607050177494205),\n",
       " (('eileen', 'fisher'), 14.585023871164207),\n",
       " (('libraries', 'inspire'), 14.570524301469092),\n",
       " (('spices', 'colonel'), 14.570524301469092),\n",
       " (('50s', '60s'), 14.529882316971749),\n",
       " (('blitz', 'big-ticket'), 14.500134973577694),\n",
       " (('christopher', 'waller'), 14.500134973577694),\n",
       " (('fe', 'luxury-home'), 14.500134973577694),\n",
       " (('gene-edited', 'babies'), 14.500134973577694),\n",
       " (('legg', 'mason'), 14.500134973577694),\n",
       " (('miller', 'lite'), 14.500134973577694),\n",
       " (('polar', 'vortex'), 14.500134973577694),\n",
       " (('procter', 'gamble'), 14.500134973577694),\n",
       " (('yankees-red', 'sox'), 14.500134973577694),\n",
       " (('fasting', 'cure'), 14.45949298908035),\n",
       " (('naomi', 'osaka'), 14.45949298908035),\n",
       " (('nurturing', 'nature'), 14.45949298908035),\n",
       " (('jamal', 'khashoggi'), 14.414405099551814),\n",
       " (('nordic', 'baltic'), 14.400599300026784),\n",
       " (('blacture', 'edges'), 14.40059930002678),\n",
       " (('builder', 'keppel'), 14.40059930002678),\n",
       " (('c.h', 'robinson'), 14.40059930002678),\n",
       " (('generations', 'bmws'), 14.40059930002678),\n",
       " (('million-year-old', 'fossil'), 14.40059930002678),\n",
       " (('openings', 'outnumber'), 14.40059930002678),\n",
       " (('swimming', 'cove'), 14.40059930002678),\n",
       " (('targeted-ad', 'capabilities'), 14.40059930002678),\n",
       " (('updated', 'purpose'), 14.40059930002678),\n",
       " (('waller', 'judy'), 14.40059930002678),\n",
       " (('wayne', 'lapierre'), 14.384657756157758),\n",
       " (('juan', 'guaidó'), 14.36604207799041),\n",
       " (('explaining', 'diverse'), 14.36263144982776),\n",
       " (('foot', 'locker'), 14.36263144982776),\n",
       " (('pop-up', 'tents'), 14.36263144982776),\n",
       " (('pei', 'dreamed'), 14.359957315529435),\n",
       " (('hudson', 'yards'), 14.348131880132645),\n",
       " (('rafael', 'nadal'), 14.348131880132645),\n",
       " (('subsidiary', 'hid'), 14.344015771660413),\n",
       " (('jared', 'kushner'), 14.330209972135382),\n",
       " (('levi', 'strauss'), 14.307489895635301),\n",
       " (('mitch', 'mcconnell'), 14.307489895635301),\n",
       " (('coco', 'gauff'), 14.3074898956353),\n",
       " (('faux', 'grass'), 14.3074898956353),\n",
       " (('insurer', 'mgic'), 14.3074898956353),\n",
       " (('kyler', 'murray'), 14.3074898956353),\n",
       " (('libyan', 'warlord'), 14.3074898956353),\n",
       " (('marc', 'mathieu'), 14.3074898956353),\n",
       " (('martha', 'vineyard'), 14.3074898956353),\n",
       " (('no-fuss', 'renovation'), 14.3074898956353),\n",
       " (('rothschild', 'pedigree'), 14.3074898956353),\n",
       " (('suv', 'massive—and'), 14.3074898956353),\n",
       " (('seeing', 'quantum-computing'), 14.307489895635298),\n",
       " (('subzero', 'temperatures'), 14.307489895635298),\n",
       " (('traveller', 'pocket'), 14.307489895635298),\n",
       " (('judy', 'shelton'), 14.301063626475868),\n",
       " (('greg', 'lindberg'), 14.277742552241245),\n",
       " (('angela', 'merkel'), 14.263095776276845),\n",
       " (('montauk', 'enclave'), 14.263095776276845),\n",
       " (('wrestle', 'oversupply'), 14.263095776276845),\n",
       " (('supercharges', 'procurement'), 14.2444800981095),\n",
       " (('vodafone', 'supercharges'), 14.2444800981095),\n",
       " (('homebuyers', 'flock'), 14.237100567743902),\n",
       " (('drugmaker', 'lannett'), 14.220027054384959),\n",
       " (('durant', 'achilles'), 14.220027054384959),\n",
       " (('harry', 'potter'), 14.220027054384959),\n",
       " (('lunar', 'module'), 14.220027054384959),\n",
       " (('programmer', 'landed'), 14.220027054384959),\n",
       " (('schemers', 'scammers'), 14.220027054384959),\n",
       " (('charlie', 'munger'), 14.21062835638271),\n",
       " (('facilitating', 'bribes'), 14.207954222084386),\n",
       " (('avengers', 'endgame'), 14.194148422559353),\n",
       " (('christine', 'lagarde'), 14.178206878690332),\n",
       " (('long-distance', 'routes'), 14.178206878690332),\n",
       " (('conventional', 'wisdom'), 14.169986371885363),\n",
       " (('ralph', 'northam'), 14.14761855885691),\n",
       " (('0-12', 'size'), 14.137564894192987),\n",
       " (('activision', 'blizzard'), 14.137564894192987),\n",
       " (('cftc', 'commissioners'), 14.137564894192987),\n",
       " (('craig', 'brommers'), 14.137564894192987),\n",
       " (('hype', 'transitions'), 14.137564894192987),\n",
       " (('incentivizing', 'whistleblowers'), 14.137564894192987),\n",
       " (('indicts', 'norway'), 14.137564894192987),\n",
       " (('nest', 'egg'), 14.137564894192987),\n",
       " (('plant—and', 'neighbors'), 14.137564894192987),\n",
       " (('retools', 'hardware'), 14.137564894192987),\n",
       " (('sobriety', 'trending'), 14.137564894192987),\n",
       " (('sean', 'mcvay'), 14.137564894192986),\n",
       " (('kfc', 'spices'), 14.111092682831794),\n",
       " (('williamson', 'ripped'), 14.08509747429885),\n",
       " (('ken', 'griffin'), 14.082523530635024),\n",
       " (('pat', 'shanahan'), 14.078671205139418),\n",
       " (('ab', 'inbev'), 14.059562382191714),\n",
       " (('feminist', 'capitalist'), 14.059562382191714),\n",
       " (('four-letter', 'word'), 14.059562382191714),\n",
       " (('glenn', 'ruffenach'), 14.059562382191714),\n",
       " (('iced', 'tea'), 14.059562382191714),\n",
       " (('mario', 'draghi'), 14.059562382191714),\n",
       " (('acquitted', 'euribor'), 14.05956238219171),\n",
       " (('ad-tech', 'in-housing'), 14.05956238219171),\n",
       " (('singapore', 'temasek'), 14.05956238219171),\n",
       " (('carl', 'icahn'), 14.044455489801505),\n",
       " (('cookies', 'suitcases'), 14.044455489801505),\n",
       " (('marc', 'benioff'), 14.044455489801505),\n",
       " (('mick', 'mulvaney'), 14.044455489801505),\n",
       " (('yellow', 'vests'), 14.044455489801505),\n",
       " (('lucky', 'charms'), 14.038029220642073),\n",
       " (('zion', 'williamson'), 14.027381976442564),\n",
       " (('backyard', 'camping'), 14.014708146407452),\n",
       " (('pork', 'chops'), 14.014708146407452),\n",
       " (('bristol-myers', 'squibb'), 13.985561800747938),\n",
       " (('ilhan', 'omar'), 13.985561800747938),\n",
       " (('shipbuilders', 'casting'), 13.985561800747938),\n",
       " (('aesthetics—and', 'fcc'), 13.985561800747936),\n",
       " (('goodwill', 'amortization'), 13.985561800747936),\n",
       " (('jack', 'dorsey'), 13.985561800747936),\n",
       " (('nas', 'x'), 13.985561800747936),\n",
       " (('omar', 'al-bashir'), 13.985561800747936),\n",
       " (('slashed', '9,700'), 13.985561800747936),\n",
       " (('quantum-computing', 'dividends'), 13.985561800747934),\n",
       " (('rig', 'builder'), 13.985561800747934),\n",
       " (('nadella', 'airpods'), 13.973589159081858),\n",
       " (('kawhi', 'leonard'), 13.972099540941375),\n",
       " (('seizes', 'msc'), 13.954973480914516),\n",
       " (('stormy', 'daniels'), 13.930520437189974),\n",
       " (('bin', 'laden'), 13.91517247285654),\n",
       " (('dangers', 'automatic-payment'), 13.91517247285654),\n",
       " (('ghana', 'exposed'), 13.91517247285654),\n",
       " (('jason', 'wu'), 13.91517247285654),\n",
       " (('robinson', 'worldwide'), 13.91517247285654),\n",
       " (('billion-dollar', 'shapewear'), 13.915172472856536),\n",
       " (('anheuser-busch', 'inbev'), 13.907559288746665),\n",
       " (('gene', 'therapy'), 13.898098959497597),\n",
       " (('jackie', 'martha'), 13.892452396356454),\n",
       " (('angels', 'pitcher'), 13.874530488359191),\n",
       " (('benioff', 'salesforce'), 13.874530488359191),\n",
       " (('lease', 'floors'), 13.874530488359191),\n",
       " (('bangladesh', 'supplying'), 13.848058276998003),\n",
       " (('citibank', 'fined'), 13.848058276998001),\n",
       " (('lemongrass', 'chicken'), 13.848058276998001),\n",
       " (('mathieu', 'departs'), 13.848058276998001),\n",
       " (('d.c', 'elites'), 13.822063068465058),\n",
       " (('garlic', 'festival'), 13.822063068465058),\n",
       " (('sexually', 'abusing'), 13.818554282687918),\n",
       " (('kentucky', 'derby'), 13.796527976357918),\n",
       " (('sexually', 'abused'), 13.796527976357918),\n",
       " (('ski', 'slopes'), 13.796527976357918),\n",
       " (('steal', 'exam'), 13.792916722805543),\n",
       " (('al', 'qaeda'), 13.783927939578287),\n",
       " (('bruce', 'willis'), 13.783927939578287),\n",
       " (('fattah', 'al'), 13.783927939578287),\n",
       " (('santa', 'fe'), 13.783927939578287),\n",
       " (('warner', 'bros.'), 13.783927939578287),\n",
       " (('confront', 'defecation'), 13.783927939578286),\n",
       " (('jony', 'ive'), 13.783927939578286),\n",
       " (('horror', 'delight'), 13.783927939578284),\n",
       " (('bin', 'salman'), 13.777668949106605),\n",
       " (('bribes', 'ecuador'), 13.748522603447089),\n",
       " (('exercises', '50s'), 13.722527394914145),\n",
       " (('invests', 'influencer'), 13.722527394914145),\n",
       " (('productivity', 'paradox'), 13.722527394914145),\n",
       " (('maps', 'life360'), 13.722527394914144),\n",
       " (('cowtown', 'reputation'), 13.722527394914142),\n",
       " (('drew', 'brees'), 13.722527394914142),\n",
       " (('duplex', 'rothschild'), 13.722527394914142),\n",
       " (('flock', 'fort'), 13.722527394914142),\n",
       " (('fortnite', 'harnessed'), 13.722527394914142),\n",
       " (('herman', 'cain'), 13.722527394914142),\n",
       " (('merging', 'shipbuilders'), 13.722527394914142),\n",
       " (('offshore', 'rig'), 13.722527394914142),\n",
       " (('pitbull', 'horizon'), 13.722527394914142),\n",
       " (('secluded', 'swimming'), 13.722527394914142),\n",
       " (('seltzer', 'crushing'), 13.722527394914142),\n",
       " (('sneak', 'peek'), 13.722527394914142),\n",
       " (('tourist', 'mobs'), 13.722527394914142),\n",
       " (('viceland', 'pivot'), 13.722527394914142),\n",
       " (('beto', 'rourke'), 13.668450722158347),\n",
       " (('figures', 'reselling'), 13.663633705860576),\n",
       " (('crazy', 'nickname'), 13.663633705860573),\n",
       " (('sunny', 'cannes'), 13.663633705860573),\n",
       " (('unsafe', 'mislabeled'), 13.663633705860573),\n",
       " (('brett', 'kavanaugh'), 13.652138067022744),\n",
       " (('keppel', 'ups'), 13.652138067022744),\n",
       " (('teradyne', 'appoints'), 13.652138067022744),\n",
       " (('bed', 'bath'), 13.64452488291287),\n",
       " (('trove', 'pricey'), 13.644524882912869),\n",
       " (('martha', 'stewart'), 13.629417990522661),\n",
       " (('alexander', 'acosta'), 13.622991721363231),\n",
       " (('dreamed', 'designs'), 13.622991721363231),\n",
       " (('answers—coming', 'detroit'), 13.607050177494207),\n",
       " (('chris', 'cline'), 13.607050177494207),\n",
       " (('manage', 'afar'), 13.607050177494207),\n",
       " (('massive—and', 'ignored'), 13.607050177494207),\n",
       " (('merchant', 'marine'), 13.607050177494207),\n",
       " (('ride', 'hailing'), 13.607050177494207),\n",
       " (('sprawling', 'turks'), 13.607050177494207),\n",
       " (('thomas', 'jefferson'), 13.607050177494207),\n",
       " (('danica', 'patrick'), 13.607050177494205),\n",
       " (('renting', 'scooters'), 13.607050177494205),\n",
       " (('ai-based', 'solutions'), 13.585023871164207),\n",
       " (('cure', 'fad'), 13.585023871164207),\n",
       " (('fried', 'chicken'), 13.585023871164207),\n",
       " (('janet', 'jackson'), 13.585023871164207),\n",
       " (('vortex', 'blasts'), 13.585023871164207),\n",
       " (('sandy', 'hook'), 13.567950357805268),\n",
       " (('sarah', 'jessica'), 13.567950357805268),\n",
       " (('devised', 'formula'), 13.561535518241836),\n",
       " (('larry', 'culp'), 13.561535518241836),\n",
       " (('larry', 'nassar'), 13.561535518241836),\n",
       " (('loads', 'goals'), 13.561535518241836),\n",
       " (('ripped', 'sneaker'), 13.561535518241836),\n",
       " (('seem', 'hip'), 13.552602393471833),\n",
       " (('high-sulfur', 'coal'), 13.55260239347183),\n",
       " (('narendra', 'modi'), 13.55260239347183),\n",
       " (('pros', 'cons'), 13.55260239347183),\n",
       " (('yield', 'curve'), 13.55260239347183),\n",
       " (('facial', 'recognition'), 13.538102823776715),\n",
       " (('avis', 'revamps'), 13.526130182110638),\n",
       " (('monster', 'beverage'), 13.519587336243868),\n",
       " (('pete', 'buttigieg'), 13.51207458688922),\n",
       " (('background', 'checks'), 13.509533671579945),\n",
       " (('iron', 'ore'), 13.500134973577696),\n",
       " (('lee', 'iacocca'), 13.500134973577696),\n",
       " (('cook', 'photo-free'), 13.500134973577694),\n",
       " (('crystal', 'ball'), 13.500134973577694),\n",
       " (('dare', 'cook'), 13.500134973577694),\n",
       " (('ebay', 'spork'), 13.500134973577694),\n",
       " (('expands', 'diversity-and-inclusion'), 13.500134973577694),\n",
       " (('golan', 'heights'), 13.500134973577694),\n",
       " (('fraudulent', 'bids'), 13.491572960074269),\n",
       " (('rough', 'transcript'), 13.483061460218751),\n",
       " (('houthi', 'rebels'), 13.474599881470557),\n",
       " (('bathrooms', 'lots'), 13.45949298908035),\n",
       " (('nature', 'architecture'), 13.45949298908035),\n",
       " (('scout', 'cookies'), 13.45949298908035),\n",
       " (('laquan', 'mcdonald'), 13.449508900507727),\n",
       " (('transitions', 'something'), 13.449508900507727),\n",
       " (('meerson', 'watches'), 13.449508900507725),\n",
       " (('jessica', 'parker'), 13.44241947572141),\n",
       " (('alex', 'rodriguez'), 13.437125176051895),\n",
       " (('philip', 'morris'), 13.436004636157978),\n",
       " (('r.', 'kelly'), 13.433020777719157),\n",
       " (('yearbook', 'photo'), 13.433020777719157),\n",
       " (('madison', 'avenue'), 13.429745645686296),\n",
       " (('winners', 'losers'), 13.42135786019358),\n",
       " (('writing', 'obituary'), 13.400599300026784),\n",
       " (('antonio', 'brown'), 13.400599300026782),\n",
       " (('a-listers', 'billionaires'), 13.40059930002678),\n",
       " (('computer', 'programmer'), 13.40059930002678),\n",
       " (('enterprise-tech', 'ipos'), 13.40059930002678),\n",
       " (('eric', 'holder'), 13.40059930002678),\n",
       " (('grass', 'delivers'), 13.40059930002678),\n",
       " (('hi', 'mom'), 13.40059930002678),\n",
       " (('pallets', 'mom'), 13.40059930002678),\n",
       " (('recruiters', 'cast'), 13.40059930002678),\n",
       " (('silver', 'lining'), 13.40059930002678),\n",
       " (('tayyip', 'erdogan'), 13.40059930002678),\n",
       " (('mohammed', 'bin'), 13.38465775615776),\n",
       " (('galaxy', 'fold'), 13.37203014783001),\n",
       " (('concierge', 'auctions'), 13.368890440299442),\n",
       " (('noncompete', 'agreements'), 13.36263144982776),\n",
       " (('trucker', 'work-hour'), 13.353293585248425),\n",
       " (('brokers', 'wrangle'), 13.353293585248423),\n",
       " (('trucker', 'roadrunner'), 13.353293585248423),\n",
       " (('vary', 'pump'), 13.348131880132646),\n",
       " (('eric', 'garner'), 13.333485104168243),\n",
       " (('ball', 'predict'), 13.330209972135382),\n",
       " (('capitalist', 'matt'), 13.322596788025507),\n",
       " (('fossil', 'ethiopia'), 13.313136458776441),\n",
       " (('info', 'money-laundering'), 13.307489895635301),\n",
       " (('highway', 'backers'), 13.3074898956353),\n",
       " (('journalist', 'ghana'), 13.3074898956353),\n",
       " (('adult', 'kid'), 13.307489895635298),\n",
       " (('colonel', 'dating'), 13.307489895635298),\n",
       " (('floors', 'virtual'), 13.289567987638037),\n",
       " (('programmers', 'unveil'), 13.289567987638037),\n",
       " (('shifted', 'dramatically'), 13.289567987638037),\n",
       " (('whistleblowers', 'weakened'), 13.289567987638037),\n",
       " (('andy', 'warhol'), 13.285122082606845),\n",
       " (('casts', 'chill'), 13.26935476674853),\n",
       " (('climbing', 'everest'), 13.263095776276845),\n",
       " (('dan', 'neil'), 13.263095776276845),\n",
       " (('kevin', 'mcaleenan'), 13.263095776276845),\n",
       " (('kevin', 'spacey'), 13.263095776276845),\n",
       " (('lions', 'gate'), 13.263095776276845),\n",
       " (('martin', 'luther'), 13.263095776276845),\n",
       " (('midwest', 'subzero'), 13.263095776276845),\n",
       " (('religious', 'observances'), 13.263095776276845),\n",
       " (('96', 'mph'), 13.24859620658173),\n",
       " (('mph', 'viral'), 13.24859620658173),\n",
       " (('student-loan', 'scams'), 13.24859620658173),\n",
       " (('nobel', 'prize'), 13.2444800981095),\n",
       " (('emmanuel', 'macron'), 13.237816367828486),\n",
       " (('seem', 'odd'), 13.237100567743902),\n",
       " (('tops', 'consumer-staples'), 13.237100567743902),\n",
       " (('monitoring', 'stations'), 13.228538554240476),\n",
       " (('aoki', 'discusses'), 13.22002705438496),\n",
       " (('generic', 'drugmaker'), 13.22002705438496),\n",
       " (('mixed', 'reactions'), 13.22002705438496),\n",
       " (('adam', 'levine'), 13.220027054384959),\n",
       " (('arab', 'emirates'), 13.220027054384959),\n",
       " (('discusses', 'post-nascar'), 13.220027054384959),\n",
       " (('kaepernick', 'buzzsaw'), 13.220027054384959),\n",
       " (('multistory', 'warehouse'), 13.220027054384959),\n",
       " ((\"o'neal\", 'discusses'), 13.220027054384959),\n",
       " (('venmo', 'etiquette'), 13.220027054384959),\n",
       " (('village', 'broken-hearted'), 13.220027054384959),\n",
       " (('luck', 'figuring'), 13.211565475636762),\n",
       " (('outlets', 'license'), 13.207954222084386),\n",
       " (('pga', 'championship'), 13.207954222084386),\n",
       " (('goals', 'surprises'), 13.19896543885713),\n",
       " (('justin', 'amash'), 13.186474494673933),\n",
       " (('jong', 'un'), 13.178206878690334),\n",
       " (('evoke', 'mission'), 13.178206878690332),\n",
       " (('harvey', 'weinstein'), 13.178206878690332),\n",
       " (('recipe', 'lemongrass'), 13.178206878690332),\n",
       " (('u.s.-asia', 'ocean'), 13.178206878690332),\n",
       " (('high-speed', 'rail'), 13.169986371885365),\n",
       " (('fcc', 'bullying'), 13.137564894192987),\n",
       " (('hillary', 'clinton'), 13.137564894192987),\n",
       " (('las', 'vegas'), 13.137564894192987),\n",
       " (('notre', 'dame'), 13.137564894192987),\n",
       " (('unilever', 'invests'), 13.137564894192987),\n",
       " (('vegas', 'paydays'), 13.137564894192987),\n",
       " (('rudy', 'giuliani'), 13.137564894192986),\n",
       " (('hyundai', 'merchant'), 13.121623350323965),\n",
       " (('mediterranean', 'islands'), 13.120491380834046),\n",
       " (('threw', '96'), 13.111092682831792),\n",
       " (('brews', 'buzz'), 13.104549836965022),\n",
       " (('steve', 'aoki'), 13.098036530006352),\n",
       " (('berkshire', 'hathaway'), 13.098036530006349),\n",
       " (('j.b.', 'hunt'), 13.098036530006349),\n",
       " (('louis', 'vuitton'), 13.098036530006349),\n",
       " (('corporate-led', 'investigations'), 13.098036530006347),\n",
       " (('401', 'k'), 13.0944961723011),\n",
       " (('ncaa', 'tournament'), 13.0944961723011),\n",
       " (('fannie', 'mae'), 13.090259179414632),\n",
       " (('version', 'eden'), 13.09025917941463),\n",
       " (('steven', 'mnuchin'), 13.085097474298852),\n",
       " (('unsteady', 'grip'), 13.08509747429885),\n",
       " (('gaza', 'strip'), 13.082523530635024),\n",
       " (('18-year', 'tenure'), 13.078671205139418),\n",
       " (('girl', 'scout'), 13.06717556630159),\n",
       " (('underage', 'girls'), 13.06717556630159),\n",
       " (('tom', 'brady'), 13.062485777398479),\n",
       " (('fred', 'smith'), 13.059562382191714),\n",
       " (('paparazzi-proofed', 'malibu'), 13.059562382191714),\n",
       " (('mae', 'freddie'), 13.044455489801505),\n",
       " (('farther', 'apart'), 13.034471401228881),\n",
       " (('figure', 'skating'), 13.034471401228881),\n",
       " (('tiger', 'woods'), 13.023696929634708),\n",
       " (('rifle', 'association'), 13.022087676773051),\n",
       " (('sri', 'lanka'), 13.022087676773051),\n",
       " (('jussie', 'smollett'), 13.02208767677305),\n",
       " (('wealthy', 'absorb'), 13.02208767677305),\n",
       " (('cuban', 'state-run'), 13.014708146407454),\n",
       " (('task', 'explaining'), 13.014708146407454),\n",
       " (('brew', 'desk'), 13.000061370443053),\n",
       " (('disturb', 'sleep'), 13.000061370443053),\n",
       " (('lockheed', 'martin'), 13.000061370443053),\n",
       " (('salesforce', 'sounds'), 13.000061370443053),\n",
       " (('thanks', 'skiing'), 13.000061370443053),\n",
       " (('plug-in', 'hybrid'), 12.99763463304851),\n",
       " (('augmented', 'reality'), 12.985561800747936),\n",
       " (('ca', \"n't\"), 12.985561800747936),\n",
       " (('kamala', 'harris'), 12.985561800747936),\n",
       " ((\"'this\", \"n't\"), 12.985561800747934),\n",
       " (('marketers', 'fret'), 12.985561800747934),\n",
       " (('charles', 'schwab'), 12.956992648551166),\n",
       " (('xi', 'jinping'), 12.949937891017216),\n",
       " (('marketplace', 'stockx'), 12.944919816250593),\n",
       " (('uphill', 'santa'), 12.935931033023337),\n",
       " (('choices', 'dull'), 12.928978272381569),\n",
       " (('toy', 'honda'), 12.926668111694367),\n",
       " (('exposed', 'fifa'), 12.91517247285654),\n",
       " (('one-pot', 'recipe'), 12.91517247285654),\n",
       " (('quantum', 'computing'), 12.915172472856538),\n",
       " (('method', 'measuring'), 12.915172472856536),\n",
       " (('u.s.-backed', 'kurds'), 12.915172472856536),\n",
       " (('guy', 'threw'), 12.909458821662144),\n",
       " (('dj', 'steve'), 12.905391452063956),\n",
       " (('journalist', 'jamal'), 12.892452396356454),\n",
       " (('al', 'sisi'), 12.890843143494799),\n",
       " (('frank', 'lloyd'), 12.890843143494799),\n",
       " (('adobe', 'explain'), 12.884584153023116),\n",
       " (('puerto', 'rico'), 12.881225140933203),\n",
       " (('misplaced', 'coffee'), 12.875644108669901),\n",
       " (('andrew', 'clarke'), 12.874530488359191),\n",
       " (('permanent', 'auto-parts'), 12.874530488359191),\n",
       " (('yesterday', 'hype'), 12.874530488359191),\n",
       " (('corn', 'syrup'), 12.870084583328001),\n",
       " (('balance', 'sheet'), 12.848058276998),\n",
       " (('cold', 'war-era'), 12.848058276998),\n",
       " (('differ', 'goodwill'), 12.848058276998),\n",
       " (('sounds', 'alarm'), 12.848058276998),\n",
       " (('steel', 'plant—and'), 12.848058276998),\n",
       " (('unusual', 'yankees-red'), 12.848058276998),\n",
       " (('oracle', 'co-ceo'), 12.84151543113123),\n",
       " (('foods', 'disturb'), 12.835002124172554),\n",
       " (('aetna', 'integration'), 12.822063068465058),\n",
       " (('resumes', 'iron'), 12.822063068465058),\n",
       " (('charms', 'seem'), 12.815636799305626),\n",
       " (('electronic', 'arts'), 12.815636799305626),\n",
       " (('dissecting', 'vaping'), 12.815636799305624),\n",
       " (('utah', 'entrepreneurs'), 12.815636799305624),\n",
       " (('wilbur', 'ross'), 12.815636799305624),\n",
       " (('coke', 'sort'), 12.815636799305622),\n",
       " (('bureau', 'prisons'), 12.804989555106113),\n",
       " (('toronto', 'raptors'), 12.801137229610509),\n",
       " (('closely', 'watched'), 12.799695255436603),\n",
       " (('condos', 'spin'), 12.796527976357918),\n",
       " (('newest', 'megamansions'), 12.796527976357918),\n",
       " (('dive', 'boat'), 12.78916458794443),\n",
       " (('boston', 'celtics'), 12.783927939578286),\n",
       " (('freddie', 'mac'), 12.783927939578286),\n",
       " (('lease-accounting', 'standard'), 12.783927939578286),\n",
       " (('nickname', 'wear'), 12.783927939578286),\n",
       " (('standard', '0-12'), 12.783927939578286),\n",
       " (('explain', 'blitz'), 12.777668949106605),\n",
       " (('revolutionary', 'guard'), 12.77499481480828),\n",
       " (('roundup', 'weedkiller'), 12.77499481480828),\n",
       " (('capitol', 'hill'), 12.771436995395089),\n",
       " (('trucker', 'celadon'), 12.768331084527269),\n",
       " (('auto-parts', 'supplier'), 12.768331084527267),\n",
       " (('learned', 'target-date'), 12.763169379411488),\n",
       " (('agile', 'processes'), 12.759053270939257),\n",
       " (('father', 'mustang'), 12.752901043957662),\n",
       " (('powers', 'self-healing'), 12.752901043957662),\n",
       " (('allscripts', 'reaches'), 12.75290104395766),\n",
       " (('self', 'confidence'), 12.75290104395766),\n",
       " (('speaker', 'nancy'), 12.745247471414226),\n",
       " (('trader', 'acquitted'), 12.737634287304349),\n",
       " (('mgm', 'resorts'), 12.734600227214717),\n",
       " (('comeback', 'loads'), 12.727545469680765),\n",
       " (('21st', 'century'), 12.722527394914142),\n",
       " (('deficiency', 'serious'), 12.722527394914142),\n",
       " (('financial-hub', 'status'), 12.722527394914142),\n",
       " (('hands', 'deck'), 12.722527394914142),\n",
       " (('hands', 'fingerprints'), 12.722527394914142),\n",
       " (('provide', 'shipment'), 12.722527394914142),\n",
       " (('soup', 'snacks'), 12.722527394914142),\n",
       " (('tuition', 'looming'), 12.722527394914142),\n",
       " (('spells', 'influencers'), 12.710554753248065),\n",
       " (('adam', 'neumann'), 12.705453881555204),\n",
       " (('science', 'fiction'), 12.70460548691688),\n",
       " (('beige', 'book'), 12.692780051520094),\n",
       " (('pedophile', 'doctor'), 12.692780051520092),\n",
       " (('improve', 'chip-making'), 12.69278005152009),\n",
       " (('nyse', 'float'), 12.69278005152009),\n",
       " (('lannett', 'hires'), 12.692780051520089),\n",
       " (('worldwide', 'double'), 12.692780051520089),\n",
       " (('beverly', 'hills'), 12.663633705860576),\n",
       " (('columnist', 'glenn'), 12.663633705860574),\n",
       " (('creating', 'targeted-ad'), 12.663633705860574),\n",
       " (('drue', 'heinz'), 12.663633705860574),\n",
       " (('fines', 'deloitte'), 12.663633705860574),\n",
       " (('gulf', 'oman'), 12.663633705860574),\n",
       " (('portable', 'allowing'), 12.663633705860574),\n",
       " (('heinz', 'devour'), 12.663633705860573),\n",
       " (('persian', 'gulf'), 12.663633705860573),\n",
       " (('roger', 'federer'), 12.663633705860573),\n",
       " (('startups', 'safeguarding'), 12.663633705860573),\n",
       " (('betsy', 'ross'), 12.655172127112378),\n",
       " (('recruiting', 'nets'), 12.652138067022744),\n",
       " (('develop', 'ai-based'), 12.646424415828351),\n",
       " (('medal', 'freedom'), 12.646424415828351),\n",
       " (('disaster-aid', 'package'), 12.645711797863314),\n",
       " (('hair', 'color'), 12.644524882912869),\n",
       " (('wells', 'fargo'), 12.635064553663806),\n",
       " (('baby', 'boomers'), 12.635064553663804),\n",
       " (('colin', 'kaepernick'), 12.635064553663803),\n",
       " (('fedex-amazon', 'split'), 12.635064553663803),\n",
       " (('islands', 'tourist'), 12.635064553663803),\n",
       " (('kaepernick', 'intervenes'), 12.635064553663803),\n",
       " (('prognosis', 'pain'), 12.635064553663803),\n",
       " (('porsche', '911'), 12.62299172136323),\n",
       " (('roger', 'stone'), 12.616327991082219),\n",
       " (('enter', '13th'), 12.616327991082215),\n",
       " (('currency', 'manipulator'), 12.607050177494207),\n",
       " (('lt.', 'gov'), 12.607050177494207),\n",
       " (('proceed', 'caution'), 12.593244377969175),\n",
       " (('altria', 'philip'), 12.591282861635891),\n",
       " (('standard', 'chartered'), 12.591282861635891),\n",
       " (('elites', 'sleep'), 12.585023871164207),\n",
       " (('northwestern', 'bahamas'), 12.585023871164207),\n",
       " (('again…to', 'retire'), 12.579569441072099),\n",
       " (('absorb', 'high-end'), 12.579569441072097),\n",
       " (('capitalist', 'professor'), 12.57413555502147),\n",
       " (('extended', 'robust'), 12.567950357805268),\n",
       " (('tim', 'armstrong'), 12.56752181330757),\n",
       " (('wear', 'swim'), 12.561535518241836),\n",
       " (('minimum', 'wage'), 12.560540212896472),\n",
       " (('bob', 'iger'), 12.552602393471831),\n",
       " (('guard', 'corps'), 12.552602393471831),\n",
       " (('19th', 'century'), 12.55260239347183),\n",
       " (('el', 'paso'), 12.55260239347183),\n",
       " (('fueled', 'first-quarter'), 12.55260239347183),\n",
       " (('mattel', 'lease'), 12.55260239347183),\n",
       " (('teenage', 'girls'), 12.55260239347183),\n",
       " (('pope', 'francis'), 12.542919840074491),\n",
       " (('ice', 'cream'), 12.536660849602809),\n",
       " (('product', 'tie-ins'), 12.536660849602809),\n",
       " (('taylor', 'swift'), 12.5360004261347),\n",
       " (('oct.', '31'), 12.529882316971749),\n",
       " (('campbell', 'soup'), 12.52613018211064),\n",
       " (('de', 'blasio'), 12.52613018211064),\n",
       " (('fend', 'scandals'), 12.526130182110638),\n",
       " (('older', 'southampton'), 12.520893533744493),\n",
       " (('serena', 'williams'), 12.519587336243866),\n",
       " (('st.', 'louis'), 12.513074029285194),\n",
       " (('timeline', 'interactions'), 12.507514503943295),\n",
       " (('origin', 'stories'), 12.500134973577696),\n",
       " (('sharing', 'materials'), 12.500134973577696),\n",
       " (('shopping', 'plaza'), 12.500134973577696),\n",
       " (('triton', 'goldman'), 12.500134973577696),\n",
       " (('emphasize', 'career'), 12.500134973577694),\n",
       " (('lights', 'proceed'), 12.500134973577694),\n",
       " (('beers', 'contain'), 12.497561029913868),\n",
       " (('kevin', 'durant'), 12.497561029913868),\n",
       " (('vladimir', 'putin'), 12.49370870441826),\n",
       " (('surf', 'vacation'), 12.489866638123868),\n",
       " (('flags', 'student-loan'), 12.483061460218751),\n",
       " (('beachfront', 'malibu'), 12.474599881470557),\n",
       " (('fastball', 'fast'), 12.474599881470557),\n",
       " (('august', 'recess'), 12.47098862791818),\n",
       " (('unraveling', 'zuckerberg'), 12.466187641654356),\n",
       " (('hottest', 'steaks'), 12.461999844690922),\n",
       " (('westbrook', 'plays'), 12.45949298908035),\n",
       " (('answers', 'reader'), 12.455047084049157),\n",
       " (('goldman', 'sachs'), 12.454331283964569),\n",
       " (('caicos', 'compound'), 12.449508900507727),\n",
       " (('producer', 'edf'), 12.449508900507727),\n",
       " (('ref', 'went'), 12.449508900507727),\n",
       " (('alibaba', 'confident'), 12.446402989639907),\n",
       " (('restore', 'reputation'), 12.44241947572141),\n",
       " (('initiative', 'thwart'), 12.441241284524125),\n",
       " (('learned', 'prosper'), 12.441241284524125),\n",
       " (('pasta', 'recipe'), 12.441241284524125),\n",
       " (('scams', 'learned'), 12.441241284524125),\n",
       " (('slack', 'buzzy'), 12.441241284524125),\n",
       " (('iacocca', 'father'), 12.4309729490703),\n",
       " (('wynn', 'resorts'), 12.426477931852387),\n",
       " (('james', 'harden'), 12.424846846273459),\n",
       " (('50th', 'anniversary'), 12.424846846273457),\n",
       " (('feel', 'cramped'), 12.424846846273457),\n",
       " (('hope', 'hicks'), 12.424846846273457),\n",
       " (('raymond', 'james'), 12.424846846273457),\n",
       " (('troubling', 'headwinds'), 12.421793522295905),\n",
       " (('el', 'chapo'), 12.421357860193577),\n",
       " (('fifth', 'avenue'), 12.420407781106961),\n",
       " (('el', 'salvador'), 12.415098869721895),\n",
       " (('greenwich', 'village'), 12.412672132327355),\n",
       " (('widespread', 'adoption'), 12.412672132327355),\n",
       " (('processes', 'develop'), 12.405416316324555),\n",
       " (('brasil', 'pays'), 12.400599300026782),\n",
       " (('pays', '4.1'), 12.400599300026782),\n",
       " (('slips', '43'), 12.400599300026782),\n",
       " (('worst', 'pac-12'), 12.40059930002678),\n",
       " (('roil', 'container'), 12.400599300026778),\n",
       " (('thrones', 'finale'), 12.400599300026778),\n",
       " (('crown', 'prince'), 12.399097274861548),\n",
       " (('skills', 'tomorrow'), 12.384657756157758),\n",
       " (('mountain', 'pitches'), 12.381490477079076),\n",
       " (('preakness', 'stakes'), 12.381490477079076),\n",
       " (('friends', 'envy'), 12.37203014783001),\n",
       " (('tallest', 'condo'), 12.361070935840143),\n",
       " (('edge', 'lockdown'), 12.359957315529435),\n",
       " (('fannie', 'freddie'), 12.353293585248425),\n",
       " (('sub-saharan', 'africa'), 12.353293585248423),\n",
       " (('houston', 'rockets'), 12.345557936468818),\n",
       " (('columnist', 'grades'), 12.341705610973213),\n",
       " (('lender', 'misled'), 12.34170561097321),\n",
       " (('human', 'ancestor'), 12.330209972135384),\n",
       " (('odds—are', 'side'), 12.330209972135382),\n",
       " (('rm', 'sotheby'), 12.330209972135382),\n",
       " (('250,000', 'votes'), 12.33020997213538),\n",
       " (('asking', '37.5'), 12.33020997213538),\n",
       " (('kpmg', 'bny'), 12.33020997213538),\n",
       " (('luther', 'king'), 12.33020997213538),\n",
       " (('cleaner', 'ships'), 12.326942258301473),\n",
       " (('l', 'train'), 12.326942258301473),\n",
       " (('pulling', 'betsy'), 12.322596788025507),\n",
       " ((\"n't\", 'revoke'), 12.3074898956353),\n",
       " (('highly', 'qualified'), 12.289567987638037),\n",
       " (('replaces', 'auditor'), 12.289567987638037),\n",
       " (('david', 'crosby'), 12.285122082606845),\n",
       " (('hidden', 'dangers'), 12.282904257357027),\n",
       " (('flag', 'saga'), 12.2814275990491),\n",
       " (('listed', '65'), 12.2814275990491),\n",
       " (('playing', 'catch-up'), 12.271865985904578),\n",
       " (('philip', 'roth'), 12.26935476674853),\n",
       " (('learn', 'successes'), 12.26747421678742),\n",
       " (('farm', 'belt'), 12.265830743998084),\n",
       " (('heavy-duty', 'truck'), 12.263095776276847),\n",
       " (('obituary', 'writer'), 12.263095776276847),\n",
       " (('nicolás', 'maduro'), 12.263095776276844),\n",
       " (('smarter', 'booze'), 12.263095776276844),\n",
       " (('swedbank', 'ousts'), 12.263095776276844),\n",
       " (('fiat', 'chrysler'), 12.260475076117709),\n",
       " (('follow', 'wherever'), 12.250039623451398),\n",
       " (('learning', 'capturing'), 12.250039623451398),\n",
       " (('equipment', 'pools'), 12.248596206581729),\n",
       " (('congestion', 'pricing'), 12.24379522031791),\n",
       " (('bets', 'pragmatist'), 12.241400705177528),\n",
       " (('golf-home', 'owners'), 12.241400705177528),\n",
       " (('prince', 'mohammed'), 12.237816367828486),\n",
       " (('16', 'bathrooms'), 12.237100567743902),\n",
       " (('breakthrough', 'quantum'), 12.237100567743902),\n",
       " (('avoided', 'penalties'), 12.230674298584468),\n",
       " (('dame', 'cathedral'), 12.22002705438496),\n",
       " (('ethiopia', 'reveals'), 12.22002705438496),\n",
       " (('kim', 'jong'), 12.22002705438496),\n",
       " (('deck', 'pull'), 12.220027054384959),\n",
       " (('fret', 'dark'), 12.220027054384959),\n",
       " (('mba', 'programs'), 12.220027054384959),\n",
       " (('photo-free', 'love'), 12.220027054384959),\n",
       " (('nhl', 'comeback'), 12.212972296851007),\n",
       " (('voices', 'troubled'), 12.211565475636764),\n",
       " (('purdue', 'pharma'), 12.206666676223923),\n",
       " (('winners', 'circle'), 12.19896543885713),\n",
       " (('cfos', 'emphasize'), 12.198965438857128),\n",
       " (('gov', 'gavin'), 12.192012678215361),\n",
       " (('carlos', 'ghosn'), 12.18823433897793),\n",
       " (('bubble', 'wrap'), 12.186474494673933),\n",
       " (('currents', 'abn'), 12.186474494673933),\n",
       " (('couple', 'whimsical'), 12.178206878690332),\n",
       " (('officers', 'wounded'), 12.178206878690332),\n",
       " (('assist', 'turnaround'), 12.17820687869033),\n",
       " (('repeat', 'champion'), 12.1740907702181),\n",
       " (('arctic', 'route'), 12.163560102725933),\n",
       " (('msc', 'container'), 12.163560102725933),\n",
       " (('ross', 'perot'), 12.163560102725933),\n",
       " (('writer', 'writing'), 12.163560102725933),\n",
       " (('asylum', 'seekers'), 12.157742776130616),\n",
       " (('malta', 'path'), 12.157742776130616),\n",
       " (('perils—of', 'living'), 12.157742776130615),\n",
       " (('morgan', 'stanley'), 12.149637726493562),\n",
       " (('vodafone', 'enters'), 12.147618558856909),\n",
       " (('hurricane', 'dorian'), 12.14731287074455),\n",
       " (('agile', 'soup'), 12.137564894192986),\n",
       " (('andrew', 'mccabe'), 12.137564894192986),\n",
       " (('barclays', 'cto'), 12.133562963635491),\n",
       " (('maritime', 'operators'), 12.131851242998595),\n",
       " (('cocaine', 'bust'), 12.127580805620363),\n",
       " (('something', 'else'), 12.127580805620363),\n",
       " (('sleep', 'aids'), 12.12559225252691),\n",
       " (('credit', 'suisse'), 12.117665336755282),\n",
       " (('ivy', 'league'), 12.117665336755282),\n",
       " (('relocate', 'bring'), 12.117665336755282),\n",
       " (('david', 'koch'), 12.115197081164531),\n",
       " (('bernie', 'sanders'), 12.11228548574463),\n",
       " (('durable', 'goods'), 12.112285485744628),\n",
       " ((\"n't\", 'chess'), 12.111092682831794),\n",
       " (('nobody', 'knows'), 12.107817550798933),\n",
       " (('app', 'downloads'), 12.098036530006349),\n",
       " (('mackinac', 'island'), 12.098036530006349),\n",
       " (('mine-waste', 'dam'), 12.098036530006349),\n",
       " (('loneliness', 'losing'), 12.094142613901859),\n",
       " (('michelle', 'obama'), 12.09247700466445),\n",
       " (('pharmacy', 'alleging'), 12.09025917941463),\n",
       " (('speed', 'freaks'), 12.09025917941463),\n",
       " (('stewardship', 'research'), 12.09025917941463),\n",
       " (('stock-fund', 'managers'), 12.09025917941463),\n",
       " (('juul', 'labs'), 12.078671205139418),\n",
       " (('mississippi', 'river'), 12.078671205139418),\n",
       " (('cannes', 'marketers'), 12.078671205139417),\n",
       " (('fetch', 'hk'), 12.078671205139417),\n",
       " (('previously', 'avoided'), 12.078671205139417),\n",
       " (('chess', 'match'), 12.075468773101072),\n",
       " (('justin', 'fairfax'), 12.070997277253996),\n",
       " (('jeff', 'bezos'), 12.068289186612295),\n",
       " (('2008', 'peak'), 12.067175566301588),\n",
       " (('side', 'hustle'), 12.067175566301588),\n",
       " (('play', 'calibrated'), 12.059562382191713),\n",
       " (('venture', 'capitalist'), 12.059562382191713),\n",
       " (('word', 'comscore'), 12.059562382191713),\n",
       " (('carriers', 'evergreen'), 12.05956238219171),\n",
       " (('swift', 'transportation'), 12.046962345412078),\n",
       " (('voice', 'assistant'), 12.046962345412078),\n",
       " (('tankers', 'rattles'), 12.043047295408698),\n",
       " (('gamble', 'kevin'), 12.040703354940396),\n",
       " (('piecemeal', 'approach'), 12.040703354940396),\n",
       " (('staff', 'viceland'), 12.040703354940396),\n",
       " (('kraft', 'heinz'), 12.035602483247532),\n",
       " (('debit', 'card'), 12.033228234378251),\n",
       " (('elon', 'musk'), 12.025280187571031),\n",
       " (('evaded', 'taxes'), 12.022087676773051),\n",
       " (('thrones', 'brews'), 12.02208767677305),\n",
       " (('patience', 'philip'), 12.018393193215308),\n",
       " (('truckers', 'wrestle'), 12.00675602301706),\n",
       " (('beauty', 'lady'), 12.000061370443051),\n",
       " (('blasts', 'midwest'), 12.000061370443051),\n",
       " (('wild', 'lions'), 12.000061370443051),\n",
       " (('calibrated', 'yet'), 11.985561800747936),\n",
       " (('cryptocurrency-based', 'payments'), 11.985561800747936),\n",
       " (('helps', 'spearhead'), 11.985561800747936),\n",
       " (('module', 'guidance'), 11.985561800747936),\n",
       " (('guidance', 'inability-to-pay'), 11.985561800747934),\n",
       " (('invaded', 'yet'), 11.985561800747934),\n",
       " (('tightens', 'grip'), 11.985561800747934),\n",
       " (('justin', 'trudeau'), 11.980023617206506),\n",
       " (('benjamin', 'netanyahu'), 11.975646331882855),\n",
       " (('turkish', 'subsidiary'), 11.974781961994694),\n",
       " (('writes', 'jason'), 11.967639892750677),\n",
       " (('must', 'nest'), 11.967639892750675),\n",
       " (('blacklists', 'cuban'), 11.964082073337485),\n",
       " (('stephen', 'curry'), 11.964082073337485),\n",
       " (('stephen', 'moore'), 11.964082073337485),\n",
       " (('anadarko', 'petroleum'), 11.959566592214992),\n",
       " (('mortgage-finance', 'giants'), 11.956992648551166),\n",
       " (('programs', 'sub-saharan'), 11.956992648551166),\n",
       " (('scottsdale', 'grows'), 11.956992648551166),\n",
       " (('spread', 'invasive'), 11.956992648551166),\n",
       " (('legacy', 'extraordinary'), 11.955814457353883),\n",
       " (('occidental', 'petroleum'), 11.953852941020598),\n",
       " (('verizon', 'expands'), 11.9526471782752),\n",
       " (('planned', 'parenthood'), 11.949937891017216),\n",
       " (('fox', 'tweak'), 11.941167681389482),\n",
       " (('seaborne', 'imports'), 11.941167681389482),\n",
       " (('services-sector', 'activity'), 11.935931033023335),\n",
       " (('tuscan', 'villa'), 11.935931033023335),\n",
       " (('living', 'redo'), 11.935350354794167),\n",
       " (('average', 'tenure'), 11.93493572767797),\n",
       " (('red', 'hat'), 11.933788151023885),\n",
       " (('ignored', 'anyone'), 11.928978272381569),\n",
       " (('pushes', 'adobe'), 11.928978272381569),\n",
       " (('water', 'cannons'), 11.926668111694369),\n",
       " (('gap', 'widened'), 11.917320939435115),\n",
       " (('13', 'nights'), 11.915172472856538),\n",
       " (('balding', 'became'), 11.915172472856538),\n",
       " (('crack', 'scam'), 11.915172472856538),\n",
       " (('kpmg', 'strengthens'), 11.915172472856538),\n",
       " (('ore', 'exports'), 11.915172472856538),\n",
       " (('paul', 'allen'), 11.915172472856536),\n",
       " (('beats', 'passwords'), 11.898098959497597),\n",
       " (('requires', 'full-time'), 11.898098959497597),\n",
       " (('limited', 'edition'), 11.898098959497595),\n",
       " (('premier', 'league'), 11.895272915418833),\n",
       " (('short', 'stewardship'), 11.894708370296822),\n",
       " (('lake', 'michigan'), 11.886026127197024),\n",
       " (('armstrong', 'backs'), 11.884584153023116),\n",
       " (('richard', 'branson'), 11.881225140933202),\n",
       " (('saban', 'wins'), 11.8812251409332),\n",
       " (('tim', 'cook'), 11.8756441086699),\n",
       " (('auditor', 'selection'), 11.874530488359191),\n",
       " (('heading', 'dayton'), 11.870084583328),\n",
       " (('westchester', 'town'), 11.867866758078181),\n",
       " (('forever', '21'), 11.866917304249318),\n",
       " (('automatic-payment', 'apps'), 11.86454639978657),\n",
       " (('measles', 'outbreak'), 11.86454639978657),\n",
       " (('rigs', 'falling'), 11.864546399786569),\n",
       " (('pump', 'pump'), 11.862705052962404),\n",
       " (('cosco', 'tankers'), 11.862475049766877),\n",
       " (('j', 'j'), 11.856278783802972),\n",
       " (('hard', 'seltzer'), 11.854630930921488),\n",
       " (('shell', 'discloses'), 11.850793244719238),\n",
       " (('iac', 'targets'), 11.848058276998001),\n",
       " (('shutdowns', 'grow'), 11.848058276998),\n",
       " (('wraps', 'camp'), 11.84478314496514),\n",
       " (('lebron', 'james'), 11.8398843455523),\n",
       " (('space', 'traveller'), 11.8317564646689),\n",
       " (('baby', 'powder'), 11.829442598830656),\n",
       " (('bright', 'spot'), 11.827709631606197),\n",
       " (('capital-gains', 'taxes'), 11.825690463969547),\n",
       " (('camp', 'sarah'), 11.818310933603948),\n",
       " (('mom', 'garage'), 11.815636799305624),\n",
       " (('land', 'lakes'), 11.815636799305622),\n",
       " (('liveramp', 'buys'), 11.815636799305622),\n",
       " (('marketers', 'fueled'), 11.815636799305622),\n",
       " (('pacific', 'heights'), 11.815636799305622),\n",
       " (('silicon', 'valley'), 11.810997904502106),\n",
       " (('apparent', 'suicide'), 11.808529912811364),\n",
       " (('ask', 'encore'), 11.805547981682041),\n",
       " (('programs', 'revamping'), 11.804989555106115),\n",
       " (('closely', 'watching'), 11.799695255436601),\n",
       " (('expressed', 'optimism'), 11.789164587944432),\n",
       " (('weakened', 'ruling'), 11.787067647108852),\n",
       " (('whole', 'foods'), 11.776108435118987),\n",
       " (('jeffrey', 'epstein'), 11.772728207836332),\n",
       " (('complaint', 'danske'), 11.768331084527267),\n",
       " (('supplier', 'incoming'), 11.768331084527267),\n",
       " (('allen', 'legacy'), 11.763169379411487),\n",
       " (('enterprise', 'collaboration'), 11.763169379411487),\n",
       " (('xinjiang', 'region'), 11.763169379411487),\n",
       " (('adviser', 'kellyanne'), 11.755973878007286),\n",
       " (('brown-forman', 'cio'), 11.75290104395766),\n",
       " (('stitch', 'fix'), 11.743721617183267),\n",
       " (('chemical', 'plant'), 11.73763428730435),\n",
       " (('hoarding', 'actually'), 11.737634287304349),\n",
       " (('abortion', 'clinic'), 11.735466450621642),\n",
       " (('full-time', 'hire'), 11.734600227214717),\n",
       " (('barrick', 'gold'), 11.730747901719111),\n",
       " (('july', 'cookout'), 11.722527394914142),\n",
       " (('measles', 'outbreaks'), 11.722527394914142),\n",
       " (('crushing', 'beer'), 11.72252739491414),\n",
       " (('steve', 'wynn'), 11.71952490675262),\n",
       " (('service-sector', 'activity'), 11.71353861168689),\n",
       " (('closed-door', 'testimony'), 11.71254330634152),\n",
       " (('focuses', 'user'), 11.71254330634152),\n",
       " (('iron', 'mountain'), 11.711639078771405),\n",
       " (('onetime', 'priciest'), 11.711300139490888),\n",
       " (('cold', 'brew'), 11.710554753248067),\n",
       " (('audit', 'outsourcing'), 11.707577053448171),\n",
       " (('audit', 'silos'), 11.707577053448171),\n",
       " (('andrew', 'luck'), 11.696992302807008),\n",
       " (('catholic', 'priests'), 11.696992302807008),\n",
       " (('england', 'wales'), 11.696992302807008),\n",
       " (('nancy', 'pelosi'), 11.696992302807004),\n",
       " (('celadon', 'hires'), 11.692780051520089),\n",
       " (('sexual', 'harassment'), 11.687881252107251),\n",
       " (('hid', 'iranian'), 11.686903485183421),\n",
       " (('samsung', 'galaxy'), 11.68397415414475),\n",
       " (('sustain', 'retailer'), 11.681050758937984),\n",
       " (('competitive', 'supporters'), 11.67813327555569),\n",
       " (('lanka', 'bombings'), 11.67813327555569),\n",
       " (('contain', 'corn'), 11.678133275555687),\n",
       " (('land', 'rover'), 11.678133275555687),\n",
       " (('old', 'wood'), 11.678133275555687),\n",
       " (('worth', 'mba'), 11.678133275555687),\n",
       " (('retire', '18-year'), 11.672678845463581),\n",
       " (('brace', 'import'), 11.663633705860574),\n",
       " (('explosion', 'chemical'), 11.663633705860574),\n",
       " (('mark', 'hurd'), 11.663633705860573),\n",
       " (('reality', 'avis'), 11.663633705860573),\n",
       " (('shapewear', 'bet'), 11.663633705860573),\n",
       " (('procurement', 'automation'), 11.659517597388344),\n",
       " (('commercial', 'actors'), 11.658832719596752),\n",
       " (('model', 'endorsing'), 11.658832719596752),\n",
       " (('sexual', 'misconduct'), 11.654328692396225),\n",
       " (('relevant', 'changing'), 11.652138067022744),\n",
       " (('renting', 'tips'), 11.64357605351932),\n",
       " (('audience', 'ratings'), 11.637423826537724),\n",
       " (('share', 'info'), 11.635064553663806),\n",
       " (('caicos', 'spread'), 11.635064553663803),\n",
       " (('cast', 'doubt'), 11.635064553663803),\n",
       " (('castle', 'river'), 11.635064553663803),\n",
       " (('l.a.', 'megamansion'), 11.635064553663803),\n",
       " (('let', 'invent'), 11.635064553663803),\n",
       " (('champions', 'league'), 11.632238509585042),\n",
       " (('vale', 'resumes'), 11.629417990522661),\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj_pmi_finder = BigramCollocationFinder.from_words(df_tokens)\n",
    "wsj_pmi_finder.apply_freq_filter(5)\n",
    "wsj_pmi_scored = wsj_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
    "wsj_pmi_scored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Abdelaziz Bouteflika: He was the President of Algeria and in 2019 he was removed from office by stepping down due to mass protests. \n",
    "2. Clayton Kershaw: A pitcher for the Los Angeles Dodgers who has struggled as of late in the post season. \n",
    "3. Nipsey Hussel: Famous Compton rapper who was killed in broad day light outside of his Marathon store. This was a huge deal for the rap and music community and touched many peopls hearts. \n",
    "4. Achilles Temdpm: Most likely in reference to the many athletes over 2019 that had achilles injuries. Just to name a few: John Wall, Kevin Durant, Dez Bryant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Text'] #X and y declared\n",
    "y=df['Coded']\n",
    "\n",
    "sent_train, sent_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=1) #Train test split\n",
    "\n",
    "vectorizer= CountVectorizer(ngram_range=(2,2), stop_words='english') #Passing in bigram arguement\n",
    "vectorizer.fit(sent_train)\n",
    "\n",
    "X_train= vectorizer.transform(sent_train)\n",
    "X_test= vectorizer.transform(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.08 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression() #Defining and fiting the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", round(score*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigrams did in fact improve our model by around 4 or 5 percent! The next step is to use neural networks and other more advanced modeling techniques. The following sections will focus on this. I am hoping to get at least a 5-10 percent increase in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest accuracy so far came from our last model, using bigrams and logistic regression. I want to try to implement a bit more complex models in the hope to improve our accruacy substantially. First I will start with a basic neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('January-October') #loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27479 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 5000 # Max number of word\n",
    "MAX_SEQUENCE_LENGTH = 250 # This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True) #Tokenizer intialization\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "word_index = tokenizer.word_index #Tokenizing into index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (35938, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Text']) #Putting data into data tensor with padding to be able to feed into model\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (35938, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['Coded']) #one hot dummy encoding for y labeling purposes \n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28750, 250) (28750, 2)\n",
      "(7188, 250) (7188, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42) #train test split\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Accuracy')\n",
    "# plt.plot(history.history['acc'], label='train')\n",
    "# plt.plot(history.history['val_acc'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\GBLS\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 100)          500000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 580,602\n",
      "Trainable params: 580,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 153s 6ms/step - loss: 0.6462 - acc: 0.6520 - val_loss: 0.6493 - val_acc: 0.6417\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 159s 6ms/step - loss: 0.6094 - acc: 0.6709 - val_loss: 0.6575 - val_acc: 0.6379\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 158s 6ms/step - loss: 0.5573 - acc: 0.7123 - val_loss: 0.6817 - val_acc: 0.6217\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 159s 6ms/step - loss: 0.5164 - acc: 0.7435 - val_loss: 0.7072 - val_acc: 0.6170\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 165s 6ms/step - loss: 0.4785 - acc: 0.7705 - val_loss: 0.7632 - val_acc: 0.6197\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 10s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.744\n",
      "  Accuracy: 0.628\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a very high loss which means we are overfitting as the training data has max accuracy of 77 percent but the highest valadation accuracy is 64%. I am going to reduce the max number of words to try to narrow down our data to the most imporant words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS=2500 #Reducing max numbder of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27479 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True) #Rerunning to fit ot model\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (35938, 250)\n",
      "Shape of label tensor: (35938, 2)\n",
      "(28750, 250) (28750, 2)\n",
      "(7188, 250) (7188, 2)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Text']) #Rerunning for the lesser max_NB_WORDS variable\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "Y = pd.get_dummies(df['Coded'])\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 808       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 331,226\n",
      "Trainable params: 331,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 187s 7ms/step - loss: 0.6481 - acc: 0.6513 - val_loss: 0.6543 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 196s 8ms/step - loss: 0.6359 - acc: 0.6528 - val_loss: 0.6517 - val_acc: 0.6390\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 175s 7ms/step - loss: 0.6100 - acc: 0.6692 - val_loss: 0.6655 - val_acc: 0.6242\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 157s 6ms/step - loss: 0.5861 - acc: 0.6876 - val_loss: 0.6707 - val_acc: 0.6092\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 157s 6ms/step - loss: 0.5675 - acc: 0.7048 - val_loss: 0.6872 - val_acc: 0.5970\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 9s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.677\n",
      "  Accuracy: 0.616\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this substantially brought down the loss score to .687 which is around .244 lower. This is a step in a right direction. I would like to add another layer to see how this affects the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 332,170\n",
      "Trainable params: 332,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 208s 8ms/step - loss: 0.6468 - acc: 0.6521 - val_loss: 0.6518 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 203s 8ms/step - loss: 0.6258 - acc: 0.6576 - val_loss: 0.6562 - val_acc: 0.6259\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 208s 8ms/step - loss: 0.5971 - acc: 0.6813 - val_loss: 0.6603 - val_acc: 0.6174\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 208s 8ms/step - loss: 0.5781 - acc: 0.6988 - val_loss: 0.6852 - val_acc: 0.6197\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 209s 8ms/step - loss: 0.5587 - acc: 0.7139 - val_loss: 0.6852 - val_acc: 0.6235\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 10s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.678\n",
      "  Accuracy: 0.631\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did not help either of our metrics and ended up decreasing the accuracy and increasing the loss slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 334,978\n",
      "Trainable params: 334,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 222s 9ms/step - loss: 0.6477 - acc: 0.6521 - val_loss: 0.6531 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 223s 9ms/step - loss: 0.6274 - acc: 0.6559 - val_loss: 0.6542 - val_acc: 0.6280\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 221s 9ms/step - loss: 0.6016 - acc: 0.6776 - val_loss: 0.6695 - val_acc: 0.6216\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 223s 9ms/step - loss: 0.5787 - acc: 0.7000 - val_loss: 0.6715 - val_acc: 0.6146\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 221s 9ms/step - loss: 0.5548 - acc: 0.7184 - val_loss: 0.7010 - val_acc: 0.6219\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 10s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.693\n",
      "  Accuracy: 0.630\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 2056      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 617,642\n",
      "Trainable params: 617,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 553s 21ms/step - loss: 0.6468 - acc: 0.6525 - val_loss: 0.6511 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 565s 22ms/step - loss: 0.6213 - acc: 0.6631 - val_loss: 0.6554 - val_acc: 0.6310\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 564s 22ms/step - loss: 0.5938 - acc: 0.6850 - val_loss: 0.6608 - val_acc: 0.6174\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 563s 22ms/step - loss: 0.5933 - acc: 0.6861 - val_loss: 0.7001 - val_acc: 0.6137\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 564s 22ms/step - loss: 0.5682 - acc: 0.7076 - val_loss: 0.6891 - val_acc: 0.6322\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 60s 8ms/step\n",
      "Test set\n",
      "  Loss: 0.679\n",
      "  Accuracy: 0.640\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reducing the number of words used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGED BACK TO 5000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27479 unique tokens.\n",
      "Shape of data tensor: (35938, 250)\n",
      "Shape of label tensor: (35938, 2)\n",
      "(28750, 250) (28750, 2)\n",
      "(7188, 250) (7188, 2)\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS=2500\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "X = tokenizer.texts_to_sequences(df['Text'])\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "Y = pd.get_dummies(df['Coded'])\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 120)               106080    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 968       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(120, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 346s 13ms/step - loss: 0.6484 - acc: 0.6478 - val_loss: 0.6535 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 528s 20ms/step - loss: 0.6383 - acc: 0.6529 - val_loss: 0.6558 - val_acc: 0.6379\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 438s 17ms/step - loss: 0.6163 - acc: 0.6677 - val_loss: 0.6606 - val_acc: 0.6083\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 438s 17ms/step - loss: 0.5959 - acc: 0.6879 - val_loss: 0.6688 - val_acc: 0.6108\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 463s 18ms/step - loss: 0.5784 - acc: 0.7022 - val_loss: 0.6805 - val_acc: 0.6125\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 23s 3ms/step\n",
      "Test set\n",
      "  Loss: 0.669\n",
      "  Accuracy: 0.629\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another layer to see if this can improve any of the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 120)               106080    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                1936      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 358,170\n",
      "Trainable params: 358,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(120, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 451s 17ms/step - loss: 0.6872 - acc: 0.5922 - val_loss: 0.6548 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 490s 19ms/step - loss: 0.6462 - acc: 0.6526 - val_loss: 0.6543 - val_acc: 0.6390\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 579s 22ms/step - loss: 0.6459 - acc: 0.6526 - val_loss: 0.6546 - val_acc: 0.6390\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 579s 22ms/step - loss: 0.6459 - acc: 0.6526 - val_loss: 0.6543 - val_acc: 0.6390\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 459s 18ms/step - loss: 0.6459 - acc: 0.6526 - val_loss: 0.6543 - val_acc: 0.6390\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 31s 4ms/step\n",
      "Test set\n",
      "  Loss: 0.647\n",
      "  Accuracy: 0.651\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another layer! So far it has little affect on both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 250, 100)          250000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 120)               106080    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 24)                2904      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 359,538\n",
      "Trainable params: 359,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(120, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(24, activation='sigmoid'))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 569s 22ms/step - loss: 0.6474 - acc: 0.6526 - val_loss: 0.6545 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 471s 18ms/step - loss: 0.6460 - acc: 0.6526 - val_loss: 0.6543 - val_acc: 0.6390\n",
      "Epoch 3/5\n",
      "25875/25875 [==============================] - 553s 21ms/step - loss: 0.6459 - acc: 0.6526 - val_loss: 0.6542 - val_acc: 0.6390\n",
      "Epoch 4/5\n",
      "25875/25875 [==============================] - 563s 22ms/step - loss: 0.6452 - acc: 0.6526 - val_loss: 0.6537 - val_acc: 0.6390\n",
      "Epoch 5/5\n",
      "25875/25875 [==============================] - 600s 23ms/step - loss: 0.6320 - acc: 0.6526 - val_loss: 0.6554 - val_acc: 0.6390\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 32s 4ms/step\n",
      "Test set\n",
      "  Loss: 0.650\n",
      "  Accuracy: 0.651\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that another layer does not improve our accuracy and really only decreases our loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will try to stack models to see if we can create a optimal model and improve both loss and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27479 unique tokens.\n",
      "Shape of data tensor: (35938, 250)\n",
      "Shape of label tensor: (35938, 2)\n",
      "(28750, 250) (28750, 2)\n",
      "(7188, 250) (7188, 2)\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS=150 #Reducing max numbder of words\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True) #Rerunning to fit ot model\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "X = tokenizer.texts_to_sequences(df['Text']) #Rerunning for the lesser max_NB_WORDS variable\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "Y = pd.get_dummies(df['Coded'])\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 250, 100)          15000     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 120)               106080    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 2)                 242       \n",
      "=================================================================\n",
      "Total params: 121,322\n",
      "Trainable params: 121,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(120, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25875 samples, validate on 2875 samples\n",
      "Epoch 1/5\n",
      "25875/25875 [==============================] - 300s 12ms/step - loss: 0.6478 - acc: 0.6521 - val_loss: 0.6553 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "25875/25875 [==============================] - 319s 12ms/step - loss: 0.6452 - acc: 0.6526 - val_loss: 0.6553 - val_acc: 0.6390\n",
      "Epoch 3/5\n",
      "12160/25875 [=============>................] - ETA: 2:41 - loss: 0.6441 - acc: 0.6512"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-475dcb8c2b4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\New folder\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a pre-trained word embedding, Glove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "text = df['Text']\n",
    "# define class labels\n",
    "labels = df['Coded']\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_text = t.texts_to_sequences(text)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 200\n",
    "padded_text = pad_sequences(encoded_text, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open(r'C:\\Users\\GBLS\\Downloads\\glove.6B\\glove.6B.50d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size,50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]= embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 250, 50)           1373950   \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 120)               82080     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 2)                 242       \n",
      "=================================================================\n",
      "Total params: 1,456,272\n",
      "Trainable params: 82,322\n",
      "Non-trainable params: 1,373,950\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=250, trainable=False))\n",
    "model.add(LSTM(120, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28750/28750 [==============================] - 250s 9ms/step - loss: 0.6496 - acc: 0.6513\n",
      "Epoch 2/50\n",
      "28750/28750 [==============================] - 246s 9ms/step - loss: 0.6471 - acc: 0.6513\n",
      "Epoch 3/50\n",
      "28750/28750 [==============================] - 264s 9ms/step - loss: 0.6466 - acc: 0.6513\n",
      "Epoch 4/50\n",
      "28750/28750 [==============================] - 322s 11ms/step - loss: 0.6461 - acc: 0.6513\n",
      "Epoch 5/50\n",
      "28750/28750 [==============================] - 318s 11ms/step - loss: 0.6460 - acc: 0.6513\n",
      "Epoch 6/50\n",
      "28750/28750 [==============================] - 309s 11ms/step - loss: 0.6457 - acc: 0.6511\n",
      "Epoch 7/50\n",
      "28750/28750 [==============================] - 310s 11ms/step - loss: 0.6452 - acc: 0.6513\n",
      "Epoch 8/50\n",
      "28750/28750 [==============================] - 303s 11ms/step - loss: 0.6447 - acc: 0.6513\n",
      "Epoch 9/50\n",
      "28750/28750 [==============================] - 295s 10ms/step - loss: 0.6437 - acc: 0.6513\n",
      "Epoch 10/50\n",
      "28750/28750 [==============================] - 306s 11ms/step - loss: 0.6429 - acc: 0.6519\n",
      "Epoch 11/50\n",
      "28750/28750 [==============================] - 318s 11ms/step - loss: 0.6416 - acc: 0.6529\n",
      "Epoch 12/50\n",
      "28750/28750 [==============================] - 279s 10ms/step - loss: 0.6388 - acc: 0.6542\n",
      "Epoch 13/50\n",
      "28750/28750 [==============================] - 301s 10ms/step - loss: 0.6371 - acc: 0.6554\n",
      "Epoch 14/50\n",
      "28750/28750 [==============================] - 310s 11ms/step - loss: 0.6340 - acc: 0.6560\n",
      "Epoch 15/50\n",
      "28750/28750 [==============================] - 336s 12ms/step - loss: 0.6300 - acc: 0.6593\n",
      "Epoch 16/50\n",
      "28750/28750 [==============================] - 315s 11ms/step - loss: 0.6275 - acc: 0.6627\n",
      "Epoch 17/50\n",
      "28750/28750 [==============================] - 291s 10ms/step - loss: 0.6223 - acc: 0.6645\n",
      "Epoch 18/50\n",
      "28750/28750 [==============================] - 274s 10ms/step - loss: 0.6191 - acc: 0.6687\n",
      "Epoch 19/50\n",
      "28750/28750 [==============================] - 255s 9ms/step - loss: 0.6144 - acc: 0.6711\n",
      "Epoch 20/50\n",
      "28750/28750 [==============================] - 255s 9ms/step - loss: 0.6116 - acc: 0.6739\n",
      "Epoch 21/50\n",
      "28750/28750 [==============================] - 307s 11ms/step - loss: 0.6068 - acc: 0.6781\n",
      "Epoch 22/50\n",
      "28750/28750 [==============================] - 323s 11ms/step - loss: 0.6043 - acc: 0.6805\n",
      "Epoch 23/50\n",
      "28750/28750 [==============================] - 277s 10ms/step - loss: 0.5995 - acc: 0.6811\n",
      "Epoch 24/50\n",
      "28750/28750 [==============================] - 299s 10ms/step - loss: 0.5958 - acc: 0.6862\n",
      "Epoch 25/50\n",
      "28750/28750 [==============================] - 308s 11ms/step - loss: 0.5921 - acc: 0.6888\n",
      "Epoch 26/50\n",
      "28750/28750 [==============================] - 255s 9ms/step - loss: 0.5872 - acc: 0.6906\n",
      "Epoch 27/50\n",
      "28750/28750 [==============================] - 250s 9ms/step - loss: 0.5828 - acc: 0.6955\n",
      "Epoch 28/50\n",
      "28750/28750 [==============================] - 249s 9ms/step - loss: 0.5814 - acc: 0.6957\n",
      "Epoch 29/50\n",
      "28750/28750 [==============================] - 246s 9ms/step - loss: 0.5750 - acc: 0.7028\n",
      "Epoch 30/50\n",
      "28750/28750 [==============================] - 253s 9ms/step - loss: 0.5736 - acc: 0.7036\n",
      "Epoch 31/50\n",
      "28750/28750 [==============================] - 253s 9ms/step - loss: 0.5687 - acc: 0.7047\n",
      "Epoch 32/50\n",
      "28750/28750 [==============================] - 253s 9ms/step - loss: 0.5657 - acc: 0.7057\n",
      "Epoch 33/50\n",
      "28750/28750 [==============================] - 255s 9ms/step - loss: 0.5631 - acc: 0.7091\n",
      "Epoch 34/50\n",
      "28750/28750 [==============================] - 340s 12ms/step - loss: 0.5576 - acc: 0.7126\n",
      "Epoch 35/50\n",
      "28750/28750 [==============================] - 342s 12ms/step - loss: 0.5562 - acc: 0.7139\n",
      "Epoch 36/50\n",
      "28750/28750 [==============================] - 301s 10ms/step - loss: 0.5540 - acc: 0.7174\n",
      "Epoch 37/50\n",
      "28750/28750 [==============================] - 298s 10ms/step - loss: 0.5521 - acc: 0.7184\n",
      "Epoch 38/50\n",
      "28750/28750 [==============================] - 329s 11ms/step - loss: 0.5476 - acc: 0.7197\n",
      "Epoch 39/50\n",
      "28750/28750 [==============================] - 3662s 127ms/step - loss: 0.5429 - acc: 0.7254\n",
      "Epoch 40/50\n",
      "28750/28750 [==============================] - 365s 13ms/step - loss: 0.5408 - acc: 0.7222\n",
      "Epoch 41/50\n",
      "28750/28750 [==============================] - 395s 14ms/step - loss: 0.5401 - acc: 0.7260\n",
      "Epoch 42/50\n",
      "28750/28750 [==============================] - 413s 14ms/step - loss: 0.5362 - acc: 0.7274\n",
      "Epoch 43/50\n",
      "28750/28750 [==============================] - 404s 14ms/step - loss: 0.5339 - acc: 0.7290\n",
      "Epoch 44/50\n",
      "28750/28750 [==============================] - 389s 14ms/step - loss: 0.5280 - acc: 0.7321\n",
      "Epoch 45/50\n",
      "28750/28750 [==============================] - 393s 14ms/step - loss: 0.5291 - acc: 0.7321\n",
      "Epoch 46/50\n",
      "28750/28750 [==============================] - 358s 12ms/step - loss: 0.5295 - acc: 0.7342\n",
      "Epoch 47/50\n",
      "28750/28750 [==============================] - 375s 13ms/step - loss: 0.5272 - acc: 0.7334\n",
      "Epoch 48/50\n",
      "28750/28750 [==============================] - 527s 18ms/step - loss: 0.5228 - acc: 0.7353\n",
      "Epoch 49/50\n",
      "28750/28750 [==============================] - 420s 15ms/step - loss: 0.5201 - acc: 0.7389\n",
      "Epoch 50/50\n",
      "28750/28750 [==============================] - 359s 13ms/step - loss: 0.5196 - acc: 0.7383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bc31167b00>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7188/7188 [==============================] - 36s 5ms/step\n",
      "Test set\n",
      "  Loss: 0.722\n",
      "  Accuracy: 0.635\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
